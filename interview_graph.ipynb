{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5952a362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59881b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from typing import Literal, Optional\n",
    "\n",
    "# 메모리 컨텍스트 저장 변수\n",
    "stored_resume: Optional[str] = None\n",
    "stored_jd: Optional[str] = None\n",
    "base_chain_inputs: Optional[dict] = None\n",
    "# RAG 벡터 스토어\n",
    "vectorstore: Optional[Chroma] = None\n",
    "# 영속 디렉토리 설정 (환경변수 또는 기본 경로)\n",
    "persist_directory = os.getenv(\n",
    "    \"CHROMA_DB_PATH\",\n",
    "    os.path.join(os.getcwd(), \"rag_agent/vectorstore/chroma_db\")\n",
    ")\n",
    "\n",
    "\n",
    "# 로컬 파일 시스템에서 context와 회사 자료 자동 로딩\n",
    "# TODO: RAG PyPDF2 -> langchain vector db\n",
    "def parse_file_to_text(file_path: str) -> str:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        content = f.read()\n",
    "    try:\n",
    "        return content.decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        if file_path.lower().endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            return \"\\n\".join(doc.page_content for doc in docs)\n",
    "        elif file_path.lower().endswith((\".docx\", \".doc\", \".txt\")):\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            return \"\\n\".join(doc.page_content for doc in docs)\n",
    "        else:\n",
    "            return content.decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "def get_company_info():\n",
    "    # 회사 자료 검색\n",
    "    retrieved = vectorstore.similarity_search(stored_jd, k=3)\n",
    "    company_info = \"\\n\".join([doc.page_content for doc in retrieved])\n",
    "    # Trim company_info to avoid exceeding model context window\n",
    "    max_company_info_length = 2000\n",
    "    if len(company_info) > max_company_info_length:\n",
    "        company_info = company_info[:max_company_info_length]\n",
    "    return company_info\n",
    "\n",
    "\n",
    "base_dir = os.path.join(os.getcwd(), \"data\")\n",
    "# 이력서 로딩\n",
    "resume_dir = os.path.join(base_dir, \"resume\")\n",
    "for fname in os.listdir(resume_dir):\n",
    "    stored_resume = parse_file_to_text(os.path.join(resume_dir, fname))\n",
    "    break\n",
    "# JD 로딩\n",
    "jd_dir = os.path.join(base_dir, \"jd\")\n",
    "for fname in os.listdir(jd_dir):\n",
    "    stored_jd = parse_file_to_text(os.path.join(jd_dir, fname))\n",
    "    break\n",
    "# 회사 자료 로딩 및 인덱싱\n",
    "company_dir = os.path.join(base_dir, \"company_infos\")\n",
    "docs = []\n",
    "for fname in os.listdir(company_dir):\n",
    "    text = parse_file_to_text(os.path.join(company_dir, fname))\n",
    "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata={\"filename\": fname}))\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory, embedding_function=embeddings\n",
    ")\n",
    "stored_company_info = get_company_info()\n",
    "base_chain_inputs = {\n",
    "    \"resume\": stored_resume,\n",
    "    \"jd\": stored_jd,\n",
    "    \"company_infos\": stored_company_info,\n",
    "}\n",
    "if docs:\n",
    "    texts = [d.page_content for d in docs]\n",
    "    metadatas = [d.metadata for d in docs]\n",
    "    vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
    "    vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7287e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "431790c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from rag_agent import ChatHistory\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str # 사용자 답변\n",
    "    answer: str # Agent 답변\n",
    "    input_type: str # 사용자 답변 유형\n",
    "    persona_id: str # 페르소나 ID\n",
    "    route_type: str # routing 결과\n",
    "    resume: str # 자소서(이력서)\n",
    "    jd: str # 채용공고\n",
    "    company: str # 회사정보 (인재상)\n",
    "    chat_history: ChatHistory # 대화내역\n",
    "    last_question: str #마지막 질문\n",
    "    \n",
    "graph_builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25b52128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langgraph.types import Command\n",
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.agent_toolkits.polygon.toolkit import PolygonToolkit\n",
    "from langchain_community.utilities.polygon import PolygonAPIWrapper\n",
    "\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.7, model_name=\"gpt-4o-mini\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8583a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_agent = create_react_agent(\n",
    "  llm,\n",
    "  tools=[],\n",
    "  prompt=\"\"\"\"\n",
    "  주어진 입력이 어떤 유형인지 판단하세요: \n",
    "  - 자소서 입력 (resume)\n",
    "  - 질문 요청 (question)\n",
    "  - 꼬리질문 요청 (followup)\n",
    "  - 모범 답변 요청 (modelAnswer)\n",
    "  - 답변 (answer)\n",
    "  - 그 외 일반 텍스트 (other)\n",
    "\n",
    "  형식: resume, question, followup, modelAnswer, answer, other 중 하나로만 답하세요.\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "def classify_input(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자 입력과, 이전 대화내용을 바탕으로 현재 입력이 어떤 형식인지 분류하고,\n",
    "    결과를 router node로 전달합니다.\n",
    "    \n",
    "    Args:\n",
    "      state (MessageState): 현재 메시지 상태를 나타내는 객체입니다.\n",
    "    \n",
    "    Returns:\n",
    "      Command: router node로 이동하기 위한 명령을 반환합니다.\n",
    "    \"\"\"\n",
    "    result = classify_agent.invoke({ input: state['query'] })\n",
    "    print(\"classify_input\", result['messages'])\n",
    "    # 분류 결과 추출 (마지막 메시지의 content가 분류값)\n",
    "    classification = result['messages'][-1].content.strip()\n",
    "    \n",
    "    # 결과 메시지를 업데이트하고 router node로 이동합니다.\n",
    "    return { \"input_type\": classification }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b38a9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_agent import PersonaService\n",
    "from rag_agent.persona.Persona import Persona, PersonaType\n",
    "from rag_agent.persona.PersonaService import PersonaInput\n",
    "\n",
    "persona_service = PersonaService.get_instance()\n",
    "persona_service.set_context(stored_resume, stored_jd)\n",
    "# 페르소나 추가 (테스트용)\n",
    "persona_service.add_persona(\n",
    "    PersonaInput(\n",
    "        name=\"Recruiter\",\n",
    "        type=\"other\",\n",
    "        interests=[\"조직 적응력\", \"인성\"],\n",
    "        communicationStyle=\"차분하고 상냥한 스타일\",\n",
    "    )\n",
    ")\n",
    "persona_service.add_persona(\n",
    "    PersonaInput(\n",
    "        name=\"CTO\",\n",
    "        type=\"developer\",\n",
    "        interests=[\"이슈 해결 과정과 Lessons Learned\"],\n",
    "        communicationStyle=\"불필요한 말은 하지 않음, 합리적이고 이성적인 스타일\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# 이거 대신 persona_service 인스턴스 사용\n",
    "# assign_persona_agent = create_react_agent(\n",
    "#     llm,\n",
    "#     tools=[],\n",
    "#     state_modifier=\"You are an HR specialist. Select the best interviewer persona for the provided context. Output only the persona id.\"\n",
    "# )\n",
    "\n",
    "def assign_persona_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    페르소나 할당 node입니다. 주어진 state를 기반으로 assign_persona 에이전트를 호출하고,\n",
    "    결과를 router node로 전달합니다.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): 현재 메시지 상태 객체.\n",
    "\n",
    "    Returns:\n",
    "        Command: router node로 이동 명령을 반환.\n",
    "    \"\"\"\n",
    "    persona_id = persona_service.invoke_agent(state)\n",
    "    print(\"assign_persona_node\", persona_id)\n",
    "    \n",
    "    return { \"persona_id\": persona_id }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bf05322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class Route(BaseModel):\n",
    "    target: Literal['generate_question', 'generate_model_answer', 'generate_followup', 'llm'] = Field(\n",
    "        description=\"The target for the query to answer\"\n",
    "    )\n",
    "\n",
    "router_system_prompt = \"\"\"\n",
    "You are an expert at routing a user's question to 'generate_question', 'generate_model_answer', 'generate_followup' or 'llm'.\n",
    "'generate_question' contains information about resume, company, jd.\n",
    "'generate_model_answer' contains information about last question and context(company, resume, jd).\n",
    "'generate_followup' contains information about last question and applicant's answer.\n",
    "if you think the question is not related to either 'generate_question', 'generate_model_answer' or 'generate_followup';\n",
    "you can route it to 'llm'.\"\"\"\n",
    "\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', router_system_prompt),\n",
    "    ('user', '{query}')\n",
    "])\n",
    "\n",
    "small_llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.7, model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "structured_router_llm = small_llm.with_structured_output(Route)\n",
    "\n",
    "def router(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    주어진 state에서 쿼리를 기반으로 적절한 경로를 결정합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 현재 에이전트의 state를 나타내는 객체입니다.\n",
    "\n",
    "    Returns:\n",
    "        Literal['generate_question', 'generate_model_answer', 'generate_followup', 'llm']: 쿼리에 따라 선택된 경로를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state['query']\n",
    "    router_chain = router_prompt | structured_router_llm \n",
    "    route = router_chain.invoke({'query': query})\n",
    "    print(\"router\", route)\n",
    "\n",
    "    return { \"route_type\": route.target }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7724dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def generate_interview_question(data: str) -> str:\n",
    "    \"\"\"자소서, JD, 회사 정보를 기반으로 면접 질문을 생성합니다.\"\"\"\n",
    "    try:\n",
    "        parsed_data = json.loads(data)\n",
    "        resume = parsed_data.get(\"resume\", \"\")\n",
    "        jd = parsed_data.get(\"jd\", \"\")\n",
    "        company = parsed_data.get(\"company\", \"\")\n",
    "        persona = parsed_data.get(\"persona\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"입력 데이터 형식이 올바르지 않습니다. JSON 형식으로 제공해주세요.\"\n",
    "\n",
    "    generation_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        다음은 지원자의 자소서, JD(직무기술서), 회사 정보, 그리고 면접관 페르소나입니다:\n",
    "\n",
    "        자기소개서:\n",
    "        {resume}\n",
    "\n",
    "        JD:\n",
    "        {jd}\n",
    "\n",
    "        회사 정보:\n",
    "        {company}\n",
    "\n",
    "        면접관 페르소나:\n",
    "        {persona}\n",
    "\n",
    "        당신은 위 페르소나를 기반으로 하는 면접관입니다.\n",
    "        다음 단계를 거쳐 면접 질문을 생성하세요:\n",
    "\n",
    "        1단계 - 분석 (Reasoning):\n",
    "        - 회사 인재상에 부합하는 성격/역량/행동을 자소서에서 얼마나 확인할 수 있는가?\n",
    "        - JD에서 요구하는 자격요건, 기술, 경험과 자소서가 얼마나 부합하는가?\n",
    "        - 부족하거나 확인이 필요한 부분은 무엇인가?\n",
    "        - 면접관 페르소나의 시각과 말투, 성격을 반영한 분석\n",
    "\n",
    "        2단계 - 질문 생성 (Acting):\n",
    "        - 1단계 분석을 바탕으로 구체적이고 답변 가능한 면접 질문 1개를 생성\n",
    "        - 면접관 페르소나의 말투와 스타일을 반영\n",
    "\n",
    "        출력 형식:\n",
    "        분석:\n",
    "        [분석 내용]\n",
    "\n",
    "        질문:\n",
    "        [생성된 면접 질문]\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    chain = generation_prompt | llm | StrOutputParser()\n",
    "    return chain.invoke({\n",
    "        \"resume\": resume, \n",
    "        \"jd\": jd, \n",
    "        \"company\": company, \n",
    "        \"persona\": persona\n",
    "    })\n",
    "\n",
    "def generation(state: AgentState) -> AgentState:\n",
    "    \"\"\"면접 질문을 생성하는 노드\"\"\"\n",
    "    try:\n",
    "        # 상태에서 필요한 정보 추출\n",
    "        resume = state.get(\"resume\", \"\")\n",
    "        jd = state.get(\"jd\", \"\")\n",
    "        company = state.get(\"company\", \"\")\n",
    "        persona = state.get(\"persona\", \"\")\n",
    "        \n",
    "        # 데이터를 JSON 형태로 준비\n",
    "        input_data = json.dumps({\n",
    "            \"resume\": resume,\n",
    "            \"jd\": jd,\n",
    "            \"company\": company,\n",
    "            \"persona\": persona\n",
    "        }, ensure_ascii=False)\n",
    "        \n",
    "        # 통합된 tool을 사용하여 질문 생성\n",
    "        result = generate_interview_question.invoke(input_data)\n",
    "        print(\"generation\", result)\n",
    "        \n",
    "        # 결과를 상태에 업데이트\n",
    "        return {\n",
    "            \"answer\": result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Generation 노드에서 오류 발생: {str(e)}\",\n",
    "            \"status\": \"error\",\n",
    "            \"messages\": state.get(\"messages\", []) + [\n",
    "                {\"role\": \"system\", \"content\": f\"오류: {str(e)}\"}\n",
    "            ]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d258b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_router(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    그래프의 조건부 엣지에서 사용할 라우팅 함수\n",
    "    \n",
    "    Args:\n",
    "        state (AgentState): 현재 상태\n",
    "        \n",
    "    Returns:\n",
    "        str: 다음 노드 이름\n",
    "    \"\"\"\n",
    "    # 상태에서 라우팅 정보 확인\n",
    "    print(state)\n",
    "    next_route = state.get('next_route', 'other')\n",
    "    \n",
    "    # 그래프 노드 이름과 매핑\n",
    "    route_mapping = {\n",
    "        'generation': 'Generation',\n",
    "        'question': 'Generation',\n",
    "        'answer': 'Generation',\n",
    "        'model_answer': 'ModelAnswer', \n",
    "        'interview_answer': 'EvaluateFollowup',\n",
    "        'other': 'SmallLLM'\n",
    "    }\n",
    "    \n",
    "    return route_mapping.get(next_route, 'SmallLLM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "308056dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1dbd671ae50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import START, END\n",
    "    \n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "# 노드 추가\n",
    "graph_builder.add_node('classify_input', classify_input)\n",
    "graph_builder.add_node('assign_persona', assign_persona_node)\n",
    "graph_builder.add_node('router', router)\n",
    "graph_builder.add_node('generation', generation)\n",
    "\n",
    "# 시작점에서 병렬 실행\n",
    "graph_builder.add_edge(START, 'classify_input')\n",
    "graph_builder.add_edge(START, 'assign_persona')\n",
    "\n",
    "# 두 병렬 노드가 완료되면 라우터로\n",
    "graph_builder.add_edge('classify_input', 'router')\n",
    "graph_builder.add_edge('assign_persona', 'router')\n",
    "\n",
    "# 생성 노드에서 종료\n",
    "graph_builder.add_edge('generation', END)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    'router',\n",
    "    conditional_router,\n",
    "    ['generation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b6f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da0cf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "966bea57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f94b3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Persona query: {'job_description': 'Python Developer\\r\\n\\r\\n토스 소속 | 정규직\\r\\n\\r\\n합류하게 될 팀에 대해 알려드려요\\r\\n\\r\\n토스의 Python Developer는 Product Designer, Product Manager와 함께 협업하고 있고, Internal Platform Team과 Toss One Team에 속해 있어요.\\r\\nInternal Platform Team에서는 토스의 문화를 지탱하고 생산성을 향상시키는 다양한 웹 서비스를 개발해요.\\r\\n손으로 작업하는 수많은 유즈 케이스를 자동화하고, 시스템이 대신 수행해줄 수 있도록 도구들을 개발해요.\\r\\n요청, 결제, 처리 프로세스를 유연하게 구성하고 처리할 수 있는 시스템을 개발해요.\\r\\n마이크로 서비스로 개별 구성되는 어드민 서비스가 빠르게 구현될 수 있도록 돕는 프레임워크를 개발해요.\\r\\n토스 내부 컨텐츠를 누구나 빠르게 작성하고 배포할 수 있는 시스템을 개발해요.\\r\\nToss One Team에서는 토스 계열사 전체를 위한 통합 상담 시스템을 개발하고 있어요.\\r\\n중요한 고객 개인정보를 다루는 서비스이기에 높은 보안 요건들을 충족시키기 위한 도구를 개발하고 있어요.\\r\\n\\r\\n합류하면 함께할 업무예요\\r\\n\\r\\n토스 계열사 전체에 배포되는 인터널 시스템의 빌드, 배포 파이프라인을 구축하고 운영해요.\\r\\n직접 만든, 50개 이상의 내부 제품 고도화와 운영 효율화를 담당하고 있어요.\\r\\n수기로 관리하던 것을 자동화하거나, 토스 외부의 시스템과 연계하여 복잡도 높은 시스템을 구성하고 개발해요.\\r\\n복잡한 정책과 디자인 시안을 기반으로 요구 사항에 적절한 설계를 빠르게 구현하고 배포해요.\\r\\n\\r\\n이런 분과 함께하고 싶어요\\r\\n\\r\\n토스팀 문화를 수호하기 위해서는 업무 툴을 만드는 사람들 또한 팀 문화를 온전히 이해해야 해요. 조직문화에 대한 관심이 많고, 관찰을 통해 문화를 주도적으로 이끌어 나가고자 하는 분을 기다리고 있어요.\\r\\n기술적인 성장을 갈구하는 마음을 가진 분과 함께하고 싶어요.\\r\\n비즈니스 요구 사항을 정확하게 이해하여, 능숙하게 데이터 모델과 API를 설계할 수 있는 분과 함께하고 싶어요.\\r\\n기술적으로 최상의 것을 선택하는 것이 아닌, 상황에 따라 최선의 선택을 같이할 수 있는 분과 함께하고 싶어요.\\r\\n\\r\\n이력서는 이렇게 작성하시는 걸 추천해요\\r\\n\\r\\n복잡도 높은 Django 기반의 Python 웹 서버 개발을 해보신 경험을 상세하게 작성해 주세요.\\r\\n능숙하게 데이터 모델과 API를 설계하실 수 있는지를 이력서와 사전과제에서 확인하고 있어요. 프로젝트 단위로 어떤 점을 기여하셨는지 기입해 주세요.\\r\\n기술로서 비즈니스의 문제를 해결했던 경험과 그 과정에서 활용하셨던 기술적 장점이 드러나는 사례를 적어주시면 좋아요.\\r\\n\\r\\n토스에서 사용하는 기술\\r\\n\\r\\nPython, Django MTV, Django REST Framework\\r\\nRedis, Memcached, Celery, Kafka\\r\\nDocker, Kubernetes, Ceph, GoCD', 'applicant_query': '\\n### 토스의 문화를 이해하고 기술로 뒷받침하는 개발자\\n\\n토스의 \"고객을 위한 최선의 선택\"이라는 문화에 깊이 공감하며, 이러한 문화가 내부 시스템에서부터 시작된다고 믿습니다. 지난 3년간 Python과 Django를 기반으로 복잡한 비즈니스 문제를 기술로 해결해온 경험을 바탕으로, 토스팀의 생산성 향상과 문화 발전에 기여하고 싶습니다.\\n\\n### 비즈니스 문제를 기술로 해결한 경험\\n\\n현재 회사에서 월 100시간이 소요되던 수동 재고 관리 프로세스를 관찰한 결과, 단순 반복 작업과 외부 시스템 간 데이터 불일치가 주요 문제였습니다. \\n\\n**문제 해결 과정:**\\n1. **문제 분석**: 업무 담당자와의 지속적인 소통을 통해 pain point 파악\\n2. **기술적 설계**: Django의 Command 패턴과 Celery를 활용한 비동기 처리 아키텍처 설계\\n3. **점진적 개선**: MVP 접근법으로 핵심 기능부터 구현하여 빠른 피드백 수집\\n4. **지속적 최적화**: 사용자 피드백을 바탕으로 기능 개선 및 확장\\n\\n결과적으로 업무 시간을 80% 단축시켰고, 더 중요한 것은 팀원들이 반복 업무에서 벗어나 더 가치 있는 일에 집중할 수 있게 되었습니다.\\n\\n### 기술적 성장에 대한 갈망\\n\\n기술적 완벽함보다는 **상황에 맞는 최선의 선택**을 추구합니다. \\n\\n예를 들어, 실시간 알림 시스템 구축 시 WebSocket이 기술적으로 더 우아한 해결책이었지만, 팀의 운영 역량과 인프라 현황을 고려하여 Polling 방식으로 시작했습니다. 이후 시스템이 안정화되고 팀 역량이 향상된 후 단계적으로 WebSocket으로 마이그레이션하여 무중단 업그레이드를 달성했습니다.\\n\\n이런 경험을 통해 **기술적 이상보다는 비즈니스 가치 창출**이 우선되어야 함을 체득했습니다.\\n\\n### 조직 문화에 대한 관심과 기여\\n\\n개발자가 만드는 도구는 결국 사람이 사용하는 것이기에, 조직 문화를 깊이 이해해야 한다고 생각합니다. \\n\\n현재 회사에서 사내 업무 자동화 플랫폼을 개발하며 다음과 같은 문화적 요소들을 고려했습니다:\\n- **투명성**: 모든 작업 과정을 슬랙으로 실시간 공유\\n- **협업**: 비개발자도 쉽게 사용할 수 있는 직관적 UI 설계\\n- **자율성**: 각 팀이 자신들의 워크플로우를 스스로 정의할 수 있는 유연한 구조\\n\\n이런 노력이 결실을 맺어 전사 생산성 향상 프로젝트에서 \\'올해의 혁신상\\'을 수상하기도 했습니다.                 \\n', 'available_personas': '[{\"id\": \"18b5a520\", \"type\": \"other\", \"name\": \"Recruiter\", \"interests\": [\"조직 적응력\", \"인성\"], \"communication_style\": \"차분하고 상냥한 스타일\"}, {\"id\": \"b8ea6474\", \"type\": \"developer\", \"name\": \"CTO\", \"interests\": [\"이슈 해결 과정과 Lessons Learned\"], \"communication_style\": \"불필요한 말은 하지 않음, 합리적이고 이성적인 스타일\"}, {\"id\": \"c671306b\", \"type\": \"other\", \"name\": \"Recruiter\", \"interests\": [\"조직 적응력\", \"인성\"], \"communication_style\": \"차분하고 상냥한 스타일\"}, {\"id\": \"c0f5cbd7\", \"type\": \"developer\", \"name\": \"CTO\", \"interests\": [\"이슈 해결 과정과 Lessons Learned\"], \"communication_style\": \"불필요한 말은 하지 않음, 합리적이고 이성적인 스타일\"}, {\"id\": \"ea47616e\", \"type\": \"other\", \"name\": \"Recruiter\", \"interests\": [\"조직 적응력\", \"인성\"], \"communication_style\": \"차분하고 상냥한 스타일\"}, {\"id\": \"165d7f2d\", \"type\": \"developer\", \"name\": \"CTO\", \"interests\": [\"이슈 해결 과정과 Lessons Learned\"], \"communication_style\": \"불필요한 말은 하지 않음, 합리적이고 이성적인 스타일\"}]'}...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify_input [AIMessage(content='other', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 102, 'total_tokens': 103, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BhEfaOhT7FTxZnbqKt9Ki3XuhnAOK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ac64468c-1096-43ed-811c-4ccebfd93794-0', usage_metadata={'input_tokens': 102, 'output_tokens': 1, 'total_tokens': 103, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.utils.pydantic.LangGraphInput'>\n",
      "[DEBUG] Agent result: {'messages': [AIMessage(content='Please provide the relevant information for me to analyze and select the most appropriate interviewer persona ID.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 109, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BhEfaZDpUWcWa2itAtDF03an0tz0C', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--336259d9-b5d2-423e-8629-bef3e72f3c29-0', usage_metadata={'input_tokens': 109, 'output_tokens': 18, 'total_tokens': 127, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "assign_persona_node None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "router target='generate_question'\n",
      "{'query': '\\n### 토스의 문화를 이해하고 기술로 뒷받침하는 개발자\\n\\n토스의 \"고객을 위한 최선의 선택\"이라는 문화에 깊이 공감하며, 이러한 문화가 내부 시스템에서부터 시작된다고 믿습니다. 지난 3년간 Python과 Django를 기반으로 복잡한 비즈니스 문제를 기술로 해결해온 경험을 바탕으로, 토스팀의 생산성 향상과 문화 발전에 기여하고 싶습니다.\\n\\n### 비즈니스 문제를 기술로 해결한 경험\\n\\n현재 회사에서 월 100시간이 소요되던 수동 재고 관리 프로세스를 관찰한 결과, 단순 반복 작업과 외부 시스템 간 데이터 불일치가 주요 문제였습니다. \\n\\n**문제 해결 과정:**\\n1. **문제 분석**: 업무 담당자와의 지속적인 소통을 통해 pain point 파악\\n2. **기술적 설계**: Django의 Command 패턴과 Celery를 활용한 비동기 처리 아키텍처 설계\\n3. **점진적 개선**: MVP 접근법으로 핵심 기능부터 구현하여 빠른 피드백 수집\\n4. **지속적 최적화**: 사용자 피드백을 바탕으로 기능 개선 및 확장\\n\\n결과적으로 업무 시간을 80% 단축시켰고, 더 중요한 것은 팀원들이 반복 업무에서 벗어나 더 가치 있는 일에 집중할 수 있게 되었습니다.\\n\\n### 기술적 성장에 대한 갈망\\n\\n기술적 완벽함보다는 **상황에 맞는 최선의 선택**을 추구합니다. \\n\\n예를 들어, 실시간 알림 시스템 구축 시 WebSocket이 기술적으로 더 우아한 해결책이었지만, 팀의 운영 역량과 인프라 현황을 고려하여 Polling 방식으로 시작했습니다. 이후 시스템이 안정화되고 팀 역량이 향상된 후 단계적으로 WebSocket으로 마이그레이션하여 무중단 업그레이드를 달성했습니다.\\n\\n이런 경험을 통해 **기술적 이상보다는 비즈니스 가치 창출**이 우선되어야 함을 체득했습니다.\\n\\n### 조직 문화에 대한 관심과 기여\\n\\n개발자가 만드는 도구는 결국 사람이 사용하는 것이기에, 조직 문화를 깊이 이해해야 한다고 생각합니다. \\n\\n현재 회사에서 사내 업무 자동화 플랫폼을 개발하며 다음과 같은 문화적 요소들을 고려했습니다:\\n- **투명성**: 모든 작업 과정을 슬랙으로 실시간 공유\\n- **협업**: 비개발자도 쉽게 사용할 수 있는 직관적 UI 설계\\n- **자율성**: 각 팀이 자신들의 워크플로우를 스스로 정의할 수 있는 유연한 구조\\n\\n이런 노력이 결실을 맺어 전사 생산성 향상 프로젝트에서 \\'올해의 혁신상\\'을 수상하기도 했습니다.                 \\n', 'input_type': 'other', 'persona_id': None, 'route_type': 'generate_question'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'SmallLLM'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m      1\u001b[39m initial_state = { \u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33m### 토스의 문화를 이해하고 기술로 뒷받침하는 개발자\u001b[39m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m \u001b[33m이런 노력이 결실을 맺어 전사 생산성 향상 프로젝트에서 \u001b[39m\u001b[33m'\u001b[39m\u001b[33m올해의 혁신상\u001b[39m\u001b[33m'\u001b[39m\u001b[33m을 수상하기도 했습니다.                 \u001b[39m\n\u001b[32m     36\u001b[39m \u001b[33m\"\"\"\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025\\NLP-team-project\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025\\NLP-team-project\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025\\NLP-team-project\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:161\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    159\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025\\NLP-team-project\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025\\NLP-team-project\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:625\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    624\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025\\NLP-team-project\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025\\NLP-team-project\\.venv\\Lib\\site-packages\\langgraph\\graph\\branch.py:174\u001b[39m, in \u001b[36mBranch._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m    172\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m    173\u001b[39m result = \u001b[38;5;28mself\u001b[39m.path.invoke(value, config)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2025\\NLP-team-project\\.venv\\Lib\\site-packages\\langgraph\\graph\\branch.py:210\u001b[39m, in \u001b[36mBranch._finish\u001b[39m\u001b[34m(self, writer, input, result, config)\u001b[39m\n\u001b[32m    207\u001b[39m     result = [result]\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ends:\n\u001b[32m    209\u001b[39m     destinations: Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]] = [\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m         r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[32m    211\u001b[39m     ]\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    213\u001b[39m     destinations = cast(Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]], result)\n",
      "\u001b[31mKeyError\u001b[39m: 'SmallLLM'",
      "During task with name 'router' and id 'f3145bd6-05d5-4e96-1191-ef09a3c2f272'"
     ]
    }
   ],
   "source": [
    "initial_state = { 'query': \"\"\"\n",
    "### 토스의 문화를 이해하고 기술로 뒷받침하는 개발자\n",
    "\n",
    "토스의 \"고객을 위한 최선의 선택\"이라는 문화에 깊이 공감하며, 이러한 문화가 내부 시스템에서부터 시작된다고 믿습니다. 지난 3년간 Python과 Django를 기반으로 복잡한 비즈니스 문제를 기술로 해결해온 경험을 바탕으로, 토스팀의 생산성 향상과 문화 발전에 기여하고 싶습니다.\n",
    "\n",
    "### 비즈니스 문제를 기술로 해결한 경험\n",
    "\n",
    "현재 회사에서 월 100시간이 소요되던 수동 재고 관리 프로세스를 관찰한 결과, 단순 반복 작업과 외부 시스템 간 데이터 불일치가 주요 문제였습니다. \n",
    "\n",
    "**문제 해결 과정:**\n",
    "1. **문제 분석**: 업무 담당자와의 지속적인 소통을 통해 pain point 파악\n",
    "2. **기술적 설계**: Django의 Command 패턴과 Celery를 활용한 비동기 처리 아키텍처 설계\n",
    "3. **점진적 개선**: MVP 접근법으로 핵심 기능부터 구현하여 빠른 피드백 수집\n",
    "4. **지속적 최적화**: 사용자 피드백을 바탕으로 기능 개선 및 확장\n",
    "\n",
    "결과적으로 업무 시간을 80% 단축시켰고, 더 중요한 것은 팀원들이 반복 업무에서 벗어나 더 가치 있는 일에 집중할 수 있게 되었습니다.\n",
    "\n",
    "### 기술적 성장에 대한 갈망\n",
    "\n",
    "기술적 완벽함보다는 **상황에 맞는 최선의 선택**을 추구합니다. \n",
    "\n",
    "예를 들어, 실시간 알림 시스템 구축 시 WebSocket이 기술적으로 더 우아한 해결책이었지만, 팀의 운영 역량과 인프라 현황을 고려하여 Polling 방식으로 시작했습니다. 이후 시스템이 안정화되고 팀 역량이 향상된 후 단계적으로 WebSocket으로 마이그레이션하여 무중단 업그레이드를 달성했습니다.\n",
    "\n",
    "이런 경험을 통해 **기술적 이상보다는 비즈니스 가치 창출**이 우선되어야 함을 체득했습니다.\n",
    "\n",
    "### 조직 문화에 대한 관심과 기여\n",
    "\n",
    "개발자가 만드는 도구는 결국 사람이 사용하는 것이기에, 조직 문화를 깊이 이해해야 한다고 생각합니다. \n",
    "\n",
    "현재 회사에서 사내 업무 자동화 플랫폼을 개발하며 다음과 같은 문화적 요소들을 고려했습니다:\n",
    "- **투명성**: 모든 작업 과정을 슬랙으로 실시간 공유\n",
    "- **협업**: 비개발자도 쉽게 사용할 수 있는 직관적 UI 설계\n",
    "- **자율성**: 각 팀이 자신들의 워크플로우를 스스로 정의할 수 있는 유연한 구조\n",
    "\n",
    "이런 노력이 결실을 맺어 전사 생산성 향상 프로젝트에서 '올해의 혁신상'을 수상하기도 했습니다.                 \n",
    "\"\"\" }\n",
    "graph.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270163b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d73f8d6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
