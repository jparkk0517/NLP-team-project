{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5952a362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59881b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:39:44 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "23:39:44 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from typing import Literal, Optional\n",
    "\n",
    "from rag_agent import ChatHistory\n",
    "\n",
    "# 메모리 컨텍스트 저장 변수\n",
    "stored_resume: Optional[str] = None\n",
    "stored_jd: Optional[str] = None\n",
    "base_chain_inputs: Optional[dict] = None\n",
    "# RAG 벡터 스토어\n",
    "vectorstore: Optional[Chroma] = None\n",
    "# 영속 디렉토리 설정 (환경변수 또는 기본 경로)\n",
    "persist_directory = os.getenv(\n",
    "    \"CHROMA_DB_PATH\",\n",
    "    os.path.join(os.getcwd(), \"rag_agent/vectorstore/chroma_db\")\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory.get_instance()\n",
    "\n",
    "# 로컬 파일 시스템에서 context와 회사 자료 자동 로딩\n",
    "# TODO: RAG PyPDF2 -> langchain vector db\n",
    "def parse_file_to_text(file_path: str) -> str:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        content = f.read()\n",
    "    try:\n",
    "        return content.decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        if file_path.lower().endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            return \"\\n\".join(doc.page_content for doc in docs)\n",
    "        elif file_path.lower().endswith((\".docx\", \".doc\", \".txt\")):\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            return \"\\n\".join(doc.page_content for doc in docs)\n",
    "        else:\n",
    "            return content.decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "def get_company_info():\n",
    "    # 회사 자료 검색\n",
    "    retrieved = vectorstore.similarity_search(stored_jd, k=3)\n",
    "    company_info = \"\\n\".join([doc.page_content for doc in retrieved])\n",
    "    # Trim company_info to avoid exceeding model context window\n",
    "    max_company_info_length = 2000\n",
    "    if len(company_info) > max_company_info_length:\n",
    "        company_info = company_info[:max_company_info_length]\n",
    "    return company_info\n",
    "\n",
    "\n",
    "base_dir = os.path.join(os.getcwd(), \"data\")\n",
    "# 이력서 로딩\n",
    "resume_dir = os.path.join(base_dir, \"resume\")\n",
    "for fname in os.listdir(resume_dir):\n",
    "    stored_resume = parse_file_to_text(os.path.join(resume_dir, fname))\n",
    "    break\n",
    "# JD 로딩\n",
    "jd_dir = os.path.join(base_dir, \"jd\")\n",
    "for fname in os.listdir(jd_dir):\n",
    "    stored_jd = parse_file_to_text(os.path.join(jd_dir, fname))\n",
    "    break\n",
    "# 회사 자료 로딩 및 인덱싱\n",
    "company_dir = os.path.join(base_dir, \"company_infos\")\n",
    "docs = []\n",
    "for fname in os.listdir(company_dir):\n",
    "    text = parse_file_to_text(os.path.join(company_dir, fname))\n",
    "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata={\"filename\": fname}))\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory, embedding_function=embeddings\n",
    ")\n",
    "stored_company_info = get_company_info()\n",
    "base_chain_inputs = {\n",
    "    \"resume\": stored_resume,\n",
    "    \"jd\": stored_jd,\n",
    "    \"company_infos\": stored_company_info,\n",
    "}\n",
    "if docs:\n",
    "    texts = [d.page_content for d in docs]\n",
    "    metadatas = [d.metadata for d in docs]\n",
    "    vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
    "    vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7287e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "431790c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from rag_agent import ChatHistory\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str # 사용자 답변\n",
    "    answer: str # Agent 답변\n",
    "    input_type: str # 사용자 답변 유형\n",
    "    persona_id: str # 페르소나 ID\n",
    "    route_type: str # routing 결과\n",
    "    resume: str # 자소서(이력서)\n",
    "    jd: str # 채용공고\n",
    "    company: str # 회사정보 (인재상)\n",
    "    chat_history: ChatHistory # 대화내역\n",
    "    last_question: str #마지막 질문        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25b52128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langgraph.types import Command\n",
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.agent_toolkits.polygon.toolkit import PolygonToolkit\n",
    "from langchain_community.utilities.polygon import PolygonAPIWrapper\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.7, model_name=\"gpt-4o-mini\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8583a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "def classify_input(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자 입력을 바탕으로 현재 입력이 어떤 형식인지 분류하고,\n",
    "    결과를 router node로 전달합니다.\n",
    "    \n",
    "    Args:\n",
    "      state (AgentState): 현재 메시지 상태를 나타내는 객체입니다.\n",
    "    \n",
    "    Returns:\n",
    "      Command: router node로 이동하기 위한 명령을 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state.get(\"query\", \"\")\n",
    "    print(\"classify_input > query >\", query)\n",
    "    \n",
    "    classify_prompt = PromptTemplate.from_template(\"\"\"\n",
    "주어진 query와 chat_history를 바탕으로 입력이이 어떤 유형인지 판단하세요: \n",
    "- 면접질문 요청 (question)\n",
    "- 꼬리질문 요청 (followup)\n",
    "- 모범답변 요청 (modelAnswer)\n",
    "- 답변 (answer)\n",
    "- 그 외 면접과 관련 없는 텍스트 (other)\n",
    "\n",
    "\n",
    "사용자 입력:\n",
    "{query}\n",
    "\n",
    "형식: question, followup, modelAnswer, answer, other 중 하나로만 답하세요.\n",
    "\"\"\")\n",
    "    \n",
    "    router_chain = classify_prompt | llm | StrOutputParser() \n",
    "    result = router_chain.invoke({'query': query})\n",
    "    \n",
    "    print(\"classify_input\", result)\n",
    "    # # 분류 결과 추출 (마지막 메시지의 content가 분류값)\n",
    "    # classification = result['messages'][-1].content.strip()\n",
    "    \n",
    "    # 결과 메시지를 업데이트하고 router node로 이동합니다.\n",
    "    return { \n",
    "        \"input_type\": result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b38a9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_agent import PersonaService\n",
    "from rag_agent.persona.Persona import Persona, PersonaType\n",
    "from rag_agent.persona.PersonaService import PersonaInput\n",
    "\n",
    "persona_service = PersonaService.get_instance()\n",
    "# persona_service.set_context(stored_resume, stored_jd)\n",
    "# 페르소나 추가 (테스트용)\n",
    "persona_service.add_persona(\n",
    "    PersonaInput(\n",
    "        name=\"Recruiter\",\n",
    "        type=\"other\",\n",
    "        interests=[\"조직 적응력\", \"인성\"],\n",
    "        communicationStyle=\"차분하고 상냥한 스타일\",\n",
    "    )\n",
    ")\n",
    "persona_service.add_persona(\n",
    "    PersonaInput(\n",
    "        name=\"CTO\",\n",
    "        type=\"developer\",\n",
    "        interests=[\"이슈 해결 과정과 Lessons Learned\"],\n",
    "        communicationStyle=\"불필요한 말은 하지 않음, 합리적이고 이성적인 스타일\",\n",
    "    )\n",
    ")\n",
    "\n",
    "def assign_persona_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    페르소나 할당 node입니다. 주어진 state를 기반으로 assign_persona 에이전트를 호출하고,\n",
    "    결과를 router node로 전달합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 현재 메시지 상태 객체.\n",
    "\n",
    "    Returns:\n",
    "        Command: router node로 이동 명령을 반환.\n",
    "    \"\"\"\n",
    "    print(state)\n",
    "    resume = state.get(\"resume\", \"\")\n",
    "    jd = state.get(\"jd\", \"\")\n",
    "    company = state.get(\"company\", \"\")\n",
    "    query = state.get(\"query\", \"\")\n",
    "    last_question = state.get(\"last_question\", \"\")\n",
    "    persona_id = persona_service.invoke_agent(resume, jd, company, query, last_question)\n",
    "    print(\"assign_persona_node\", persona_id)\n",
    "    \n",
    "    return { \"persona_id\": persona_id }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bf05322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class Route(BaseModel):\n",
    "    target: Literal['question', 'model_answer', 'followup', 'llm'] = Field(\n",
    "        description=\"The target for the query to answer\"\n",
    "    )\n",
    "\n",
    "def router(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    주어진 state에서 input_type를 기반으로 적절한 경로를 결정합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 현재 에이전트의 state를 나타내는 객체입니다.\n",
    "\n",
    "    Returns:\n",
    "        Literal['question', 'model_answer', 'followup', 'llm']: 쿼리에 따라 선택된 경로를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state['query']\n",
    "    \n",
    "    router_system_prompt = \"\"\"\n",
    "You are an expert at routing a user's input type to 'question', 'model_answer', 'followup' or 'llm'.\n",
    "If the user input is 'question' route to 'question'.\n",
    "else if the user input is 'model_answer' route to 'model_answer',\n",
    "else if the user input is 'followup' route to 'followup',\n",
    "\n",
    "if you think the input is not related to either 'question', 'model_answer' or 'followup';\n",
    "you can route it to 'llm'.\"\"\"\n",
    "    \n",
    "    router_prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', router_system_prompt),\n",
    "        ('user', '{query}')\n",
    "    ])\n",
    "\n",
    "    structured_router_llm = llm.with_structured_output(Route)\n",
    "\n",
    "    router_chain = router_prompt | structured_router_llm \n",
    "    route = router_chain.invoke({'query': query})\n",
    "    print(\"router\", route)\n",
    "\n",
    "    return { \"route_type\": route }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7724dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import json\n",
    "\n",
    "def generation(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자 입력과, 이전 대화내용을 바탕으로 면접 질문을 생성하고,\n",
    "    결과를 router node로 전달합니다.\n",
    "    \n",
    "    Args:\n",
    "      state (MessageState): 현재 메시지 상태를 나타내는 객체입니다.\n",
    "    \n",
    "    Returns:\n",
    "      Command: router node로 이동하기 위한 명령을 반환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 상태에서 필요한 정보 추출\n",
    "        resume = state.get(\"resume\", \"\")\n",
    "        jd = state.get(\"jd\", \"\")\n",
    "        company = state.get(\"company\", \"\")\n",
    "        persona = state.get(\"persona\", \"\")\n",
    "                \n",
    "        generation_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"다음은 지원자의 자소서, JD(직무기술서), 회사 정보, 그리고 면접관 페르소나입니다:\n",
    "\n",
    "            자기소개서:\n",
    "            {resume}\n",
    "\n",
    "            JD:\n",
    "            {jd}\n",
    "\n",
    "            회사 정보:\n",
    "            {company}\n",
    "\n",
    "            면접관 페르소나:\n",
    "            {persona}\n",
    "\n",
    "            당신은 위 페르소나를 기반으로 하는 면접관입니다.\n",
    "            다음 단계를 거쳐 면접 질문을 생성하세요:\n",
    "\n",
    "            1단계 - 분석 (Reasoning):\n",
    "            - 회사 인재상에 부합하는 성격/역량/행동을 자소서에서 얼마나 확인할 수 있는가?\n",
    "            - JD에서 요구하는 자격요건, 기술, 경험과 자소서가 얼마나 부합하는가?\n",
    "            - 부족하거나 확인이 필요한 부분은 무엇인가?\n",
    "            - 면접관 페르소나의 시각과 말투, 성격을 반영한 분석\n",
    "\n",
    "            2단계 - 질문 생성 (Acting):\n",
    "            - 1단계 분석을 바탕으로 구체적이고 답변 가능한 면접 질문 1개를 생성\n",
    "            - 면접관 페르소나의 말투와 스타일을 반영\n",
    "\n",
    "            출력 형식:\n",
    "            [2단계에서 생성된 면접 질문]\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        chain = generation_prompt | llm | StrOutputParser()\n",
    "        result = chain.invoke({\n",
    "            \"resume\": resume, \n",
    "            \"jd\": jd, \n",
    "            \"company\": company, \n",
    "            \"persona\": persona\n",
    "        })\n",
    "        print(\"result\", result)\n",
    "        \n",
    "        # 결과를 상태에 업데이트\n",
    "        return {\n",
    "            \"answer\": result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Generation 노드에서 오류 발생: {str(e)}\",\n",
    "            \"status\": \"error\",\n",
    "            \"messages\": state.get(\"messages\", []) + [\n",
    "                {\"role\": \"system\", \"content\": f\"오류: {str(e)}\"}\n",
    "            ]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25ee20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "# from langchain_core.tools import tool\n",
    "\n",
    "# import json\n",
    "\n",
    "# class AssessmentResult(BaseModel):\n",
    "#     logicScore: int\n",
    "#     jobFitScore: int\n",
    "#     coreValueFitScore: int\n",
    "#     communicationScore: int\n",
    "#     averageScore: float\n",
    "\n",
    "# def evaluate_answer(state: AgentState) -> AgentState:\n",
    "#     \"\"\"\n",
    "#     이전 대화내용을 기반으로 사용자 입력을 평가하고\n",
    "#     결과를 router node로 전달합니다.\n",
    "    \n",
    "#     Args:\n",
    "#       state (AgentState): 현재 메시지 상태를 나타내는 객체입니다.\n",
    "    \n",
    "#     Returns:\n",
    "#       Command: router node로 이동하기 위한 명령을 반환합니다.\n",
    "#     \"\"\"\n",
    "\n",
    "#     parser = JsonOutputParser(pydantic_object=AssessmentResult)\n",
    "    \n",
    "#     # 상태에서 필요한 정보 추출\n",
    "#     resume = state.get(\"resume\", \"\")\n",
    "#     jd = state.get(\"jd\", \"\")\n",
    "#     company = state.get(\"company\", \"\")\n",
    "#     persona = state.get(\"persona\", \"\")\n",
    "#     last_question = state.get(\"last_question\", \"\")\n",
    "        \n",
    "#     assessment_prompt = PromptTemplate(\n",
    "#         input_variables=[\"resume\", \"jd\", \"company\", \"last_question\", \"answer\", \"persona\"],\n",
    "#         template=\"\"\"\n",
    "#         역할: 면접관으로서 지원자의 답변을 평가합니다.\n",
    "\n",
    "#         직무 설명:\n",
    "#         {jd}\n",
    "\n",
    "#         이력서:\n",
    "#         {resume}\n",
    "\n",
    "#         회사 정보:\n",
    "#         {company}\n",
    "\n",
    "#         질문: \n",
    "#         {last_question}\n",
    "        \n",
    "#         지원자의 답변:\n",
    "#         {answer}\n",
    "\n",
    "#         면접관 정보:\n",
    "#         {persona}\n",
    "        \n",
    "#         다음 4개 항목을 0-10점으로 평가하세요:\n",
    "#         1. 논리성 (logicScore): 답변의 논리적 일관성과 구조\n",
    "#         2. 직무적합성 (jobFitScore): JD 요구사항과의 부합도\n",
    "#         3. 핵심가치 부합성 (coreValueFitScore): 회사 가치와의 일치도\n",
    "#         4. 커뮤니케이션 능력 (communicationScore): 의사소통 명확성\n",
    "\n",
    "#         {format_instructions}\n",
    "#         \"\"\",\n",
    "#         partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "#     )\n",
    "\n",
    "#     chain = assessment_prompt | llm | parser\n",
    "    \n",
    "#     try:\n",
    "#         result = chain.invoke({\n",
    "#             \"jd\": jd,\n",
    "#             \"resume\": resume,\n",
    "#             \"company\": company,\n",
    "#             \"question\": question,\n",
    "#             \"answer\": answer,\n",
    "#             \"persona\": persona,\n",
    "#         })\n",
    "#         return result\n",
    "#     except Exception as e:\n",
    "#         return {\"error\": \"평가 중 오류가 발생했습니다.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f64d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import json\n",
    "\n",
    "def followup(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자 입력과, 이전 대화내용을 바탕으로 현재 입력에 대한 꼬리질문을 생성하고,\n",
    "    결과를 router node로 전달합니다.\n",
    "    \n",
    "    Args:\n",
    "      state (MessageState): 현재 메시지 상태를 나타내는 객체입니다.\n",
    "    \n",
    "    Returns:\n",
    "      Command: router node로 이동하기 위한 명령을 반환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 상태에서 필요한 정보 추출\n",
    "        resume = state.get(\"resume\", \"\")\n",
    "        jd = state.get(\"jd\", \"\")\n",
    "        company = state.get(\"company\", \"\")\n",
    "        persona = state.get(\"persona\", \"\")\n",
    "        chat_history = state.get(\"chat_history\", \"\")\n",
    "        query = state.get(\"query\", \"\")\n",
    "                \n",
    "        followup_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"아래는 AI 면접 시스템에서 지금까지 진행된 대화입니다:\n",
    "\n",
    "대화 이력:\n",
    "{chat_history}\n",
    "\n",
    "현재 질문에 대한 지원자의 답변:\n",
    "{query}\n",
    "\n",
    "자기소개서:\n",
    "{resume}\n",
    "\n",
    "JD:\n",
    "{jd}\n",
    "\n",
    "회사 정보:\n",
    "{company}\n",
    "\n",
    "면접관 페르소나:\n",
    "{persona}\n",
    "\n",
    "당신은 위 페르소나를 기반으로 하는 면접관입니다.\n",
    "다음 단계를 거쳐 면접 질문을 생성하세요:\n",
    "\n",
    "1단계 - 분석 (Reasoning):\n",
    "- 회사 인재상에 부합하는 성격/역량/행동을 자소서에서 얼마나 확인할 수 있는가?\n",
    "- JD에서 요구하는 자격요건, 기술, 경험과 자소서가 얼마나 부합하는가?\n",
    "- 부족하거나 확인이 필요한 부분은 무엇인가?\n",
    "- 면접관 페르소나의 시각과 말투, 성격을 반영한 분석\n",
    "\n",
    "2단계 - 질문 생성 (Follow-up Question):\n",
    "- 1단계 분석을 바탕으로 구체적이고 답변 가능한 면접 질문 1개를 생성\n",
    "- 면접관 페르소나의 말투와 스타일을 반영\n",
    "\n",
    "출력 형식:\n",
    "[생성된 꼬리 면접 질문]\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        chain = followup_prompt | llm | StrOutputParser()\n",
    "        result = chain.invoke({\n",
    "            \"resume\": resume, \n",
    "            \"jd\": jd, \n",
    "            \"company\": company, \n",
    "            \"persona\": persona,\n",
    "            \"chat_history\": chat_history,\n",
    "            \"query\": query,\n",
    "        })\n",
    "        print(\"result\", result)\n",
    "        \n",
    "        # 결과를 상태에 업데이트\n",
    "        return {\n",
    "            \"answer\": result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Followup 노드에서 오류 발생: {str(e)}\",\n",
    "            \"status\": \"error\",\n",
    "            \"messages\": state.get(\"messages\", []) + [\n",
    "                {\"role\": \"system\", \"content\": f\"오류: {str(e)}\"}\n",
    "            ]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3025ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    주어진 state에서 쿼리를 LLM에 전달하여 응답을 얻습니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 현재 에이전트의 state를 나타내는 객체입니다.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 'answer' 키를 포함하는 새로운 state를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state['query']\n",
    "    llm_chain = llm | StrOutputParser()\n",
    "    llm_answer = llm_chain.invoke(query)\n",
    "    return {'answer': llm_answer }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49b75c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import json\n",
    "\n",
    "# LLM 정의\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.7,\n",
    "    model_name=\"gpt-4\"\n",
    ")\n",
    "\n",
    "\n",
    "def model_answer(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    STAR 기법 등 구조화된 최적의 모범 답변을 생성하는 LangGraph용 노드 함수.\n",
    "    이력서, JD, 회사정보, 이전 Q&A, 질문, 페르소나 등 context를 모두 반영.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. 상태에서 정보 추출\n",
    "        resume = state.get(\"resume\", \"\")\n",
    "        jd = state.get(\"jd\", \"\")\n",
    "        company_infos = state.get(\"company\", \"\")\n",
    "        prev_question_answer_pairs = state.get(\"chat_history\", \"\")\n",
    "        question = state.get(\"last_question\", \"\")\n",
    "        persona = state.get(\"persona_id\", \"\")\n",
    "        chat_history = state.get(\"chat_history\", \"\")\n",
    "\n",
    "        # 2. 프롬프트 템플릿 정의\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "아래는 AI 면접 시스템에서 지금까지 진행된 대화입니다:\n",
    "\n",
    "상황:\n",
    "{company_infos}\n",
    "\n",
    "이력서:\n",
    "{resume}\n",
    "\n",
    "직무 설명:\n",
    "{jd}\n",
    "\n",
    "이전 질문/답변 쌍들:\n",
    "{prev_question_answer_pairs}\n",
    "\n",
    "현재 질문:\n",
    "{question}\n",
    "\n",
    "면접관 페르소나:\n",
    "{persona}\n",
    "\n",
    "대화 이력:\n",
    "{chat_history}\n",
    "\n",
    "---\n",
    "당신은 위 페르소나를 기반으로 하는 면접관입니다.\n",
    "주어진 질문에 대해 최적의 답변을 생성해야 합니다.\n",
    "이 답변은 회사의 가치관, 직무 요구사항, 그리고 이력서의 내용을 모두 고려해야 합니다.\n",
    "\n",
    "답변은 다음 형식을 따라야 합니다:\n",
    "\n",
    "1. STAR 기법을 활용한 구조화된 답변:\n",
    "   - Situation: 상황 설명\n",
    "   - Task: 해결해야 할 과제\n",
    "   - Action: 취한 행동\n",
    "   - Result: 결과와 배운 점\n",
    "\n",
    "2. 직무 관련성:\n",
    "   - JD에서 요구하는 역량과의 연관성\n",
    "   - 회사의 핵심 가치와의 부합성\n",
    "\n",
    "3. 구체성:\n",
    "   - 구체적인 숫자와 데이터 포함\n",
    "   - 실제 경험 기반의 예시\n",
    "\n",
    "4. 논리성:\n",
    "   - 명확한 인과관계\n",
    "   - 체계적인 설명\n",
    "\n",
    "답변은 한글로 생성해야 한다.\n",
    "\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        # 3. LLM 체인 실행\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        result = chain.invoke({\n",
    "            \"resume\": resume,\n",
    "            \"jd\": jd,\n",
    "            \"company_infos\": company_infos,\n",
    "            \"prev_question_answer_pairs\": prev_question_answer_pairs,\n",
    "            \"question\": question,\n",
    "            \"persona\": persona,\n",
    "            \"chat_history\": chat_history,\n",
    "        })\n",
    "\n",
    "        # 4. 결과를 state에 업데이트\n",
    "        return {\n",
    "            **state,\n",
    "            \"model_answer\": result\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": f\"model_answer_node에서 오류 발생: {str(e)}\",\n",
    "            \"status\": \"error\",\n",
    "            \"messages\": state.get(\"messages\", []) + [\n",
    "                {\"role\": \"system\", \"content\": f\"오류: {str(e)}\"}\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d258b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_router(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    그래프의 조건부 엣지에서 사용할 라우팅 함수\n",
    "    \n",
    "    Args:\n",
    "        state (AgentState): 현재 상태\n",
    "        \n",
    "    Returns:\n",
    "        str: 다음 노드 이름\n",
    "    \"\"\"\n",
    "    # 상태에서 라우팅 정보 확인\n",
    "    print(state)\n",
    "    next_route = state.get('next_route', 'other')\n",
    "    \n",
    "    # 그래프 노드 이름과 매핑\n",
    "    route_mapping = {\n",
    "        'generation': 'generation',\n",
    "        'question': 'generation',\n",
    "        'answer': 'generation',\n",
    "        'followup': 'followup',\n",
    "        'model_answer': 'ModelAnswer', \n",
    "        'interview_answer': 'EvaluateFollowup',\n",
    "        'other': 'llm'\n",
    "    }\n",
    "    \n",
    "    return route_mapping.get(next_route, 'llm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "308056dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f27dd98d60>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langgraph.graph import START, END\n",
    "    \n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "# 노드 추가\n",
    "graph_builder.add_node('classify_input', classify_input)\n",
    "graph_builder.add_node('assign_persona', assign_persona_node)\n",
    "graph_builder.add_node('router', router)\n",
    "graph_builder.add_node('generation', generation)\n",
    "graph_builder.add_node('followup', followup)\n",
    "graph_builder.add_node('llm', call_llm)\n",
    "graph_builder.add_node('model_answer', model_answer)\n",
    "\n",
    "# 시작점에서 병렬 실행\n",
    "graph_builder.add_edge(START, 'classify_input')\n",
    "graph_builder.add_edge(START, 'assign_persona')\n",
    "\n",
    "# 두 병렬 노드가 완료되면 라우터로\n",
    "graph_builder.add_edge('classify_input', 'router')\n",
    "graph_builder.add_edge('assign_persona', 'router')\n",
    "\n",
    "# 생성 노드에서 종료\n",
    "graph_builder.add_edge('generation', END)\n",
    "graph_builder.add_edge('followup', END)\n",
    "graph_builder.add_edge('llm', END)\n",
    "graph_builder.add_edge('model_answer', END)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    'router',\n",
    "    conditional_router,\n",
    "    {\n",
    "        \"generation\": \"generation\",\n",
    "        \"followup\": \"followup\",\n",
    "        \"llm\": \"llm\",\n",
    "        \"model_answer\": \"model_answer\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b6f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da0cf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "966bea57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f94b3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '떡볶이 먹고싶다 서울 떡볶이 맛집 좀 알려줘줘', 'resume': \"John Doe\\r\\nSoftware Engineer\\r\\n\\r\\nEducation:\\r\\n- Bachelor's degree in Computer Science, XYZ University\\r\\n- Master's degree in Artificial Intelligence, ABC University\\r\\n\\r\\nExperience:\\r\\n- Senior Software Engineer at Tech Corp (2020-Present)\\r\\n- Software Engineer at Startup Inc (2018-2020)\\r\\n\\r\\nSkills:\\r\\n- Python, JavaScript, React\\r\\n- Machine Learning, NLP\\r\\n- Web Development\\r\\n- Cloud Computing \", 'jd': \"Software Engineer Position\\r\\n\\r\\nRequirements:\\r\\n- Bachelor's degree in Computer Science or related field\\r\\n- 3+ years of experience in software development\\r\\n- Strong knowledge of Python and web frameworks\\r\\n- Experience with machine learning and NLP is a plus\\r\\n\\r\\nResponsibilities:\\r\\n- Develop and maintain web applications\\r\\n- Implement new features and improvements\\r\\n- Collaborate with team members\\r\\n- Write clean and maintainable code \", 'company': 'Tech Corp\\r\\n\\r\\nAbout Us:\\r\\nTech Corp is a leading technology company specializing in artificial intelligence and machine learning solutions. We are committed to innovation and excellence in everything we do.\\r\\n\\r\\nCulture:\\r\\n- Collaborative and inclusive work environment\\r\\n- Emphasis on continuous learning and growth\\r\\n- Work-life balance\\r\\n- Remote-friendly workplace\\r\\n\\r\\nBenefits:\\r\\n- Competitive salary\\r\\n- Health insurance\\r\\n- 401(k) matching\\r\\n- Flexible work hours\\r\\n- Professional development opportunities\\nTech Corp\\r\\n\\r\\nAbout Us:\\r\\nTech Corp is a leading technology company specializing in artificial intelligence and machine learning solutions. We are committed to innovation and excellence in everything we do.\\r\\n\\r\\nCulture:\\r\\n- Collaborative and inclusive work environment\\r\\n- Emphasis on continuous learning and growth\\r\\n- Work-life balance\\r\\n- Remote-friendly workplace\\r\\n\\r\\nBenefits:\\r\\n- Competitive salary\\r\\n- Health insurance\\r\\n- 401(k) matching\\r\\n- Flexible work hours\\r\\n- Professional development opportunities\\nTech Corp\\r\\n\\r\\nAbout Us:\\r\\nTech Corp is a leading technology company specializing in artificial intelligence and machine learning solutions. We are committed to innovation and excellence in everything we do.\\r\\n\\r\\nCulture:\\r\\n- Collaborative and inclusive work environment\\r\\n- Emphasis on continuous learning and growth\\r\\n- Work-life balance\\r\\n- Remote-friendly workplace\\r\\n\\r\\nBenefits:\\r\\n- Competitive salary\\r\\n- Health insurance\\r\\n- 401(k) matching\\r\\n- Flexible work hours\\r\\n- Professional development opportunities', 'chat_history': '', 'last_question': None}classify_input > query > 떡볶이 먹고싶다 서울 떡볶이 맛집 좀 알려줘줘\n",
      "\n",
      "available_personas_json [{'id': '9483958f', 'type': 'other', 'name': 'Recruiter', 'interests': ['조직 적응력', '인성'], 'communication_style': '차분하고 상냥한 스타일'}, {'id': '1fd2991f', 'type': 'developer', 'name': 'CTO', 'interests': ['이슈 해결 과정과 Lessons Learned'], 'communication_style': '불필요한 말은 하지 않음, 합리적이고 이성적인 스타일'}, {'id': 'f876fb37', 'type': 'other', 'name': 'Recruiter', 'interests': ['조직 적응력', '인성'], 'communication_style': '차분하고 상냥한 스타일'}, {'id': '0e208c93', 'type': 'developer', 'name': 'CTO', 'interests': ['이슈 해결 과정과 Lessons Learned'], 'communication_style': '불필요한 말은 하지 않음, 합리적이고 이성적인 스타일'}, {'id': '860e0f15', 'type': 'other', 'name': 'Recruiter', 'interests': ['조직 적응력', '인성'], 'communication_style': '차분하고 상냥한 스타일'}, {'id': '34215d83', 'type': 'developer', 'name': 'CTO', 'interests': ['이슈 해결 과정과 Lessons Learned'], 'communication_style': '불필요한 말은 하지 않음, 합리적이고 이성적인 스타일'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:43:11 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify_input other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:43:14 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "c:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1669: UserWarning: Cannot use method='json_schema' with model gpt-4 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assign_persona_node Thought: The applicant is a software engineer with experience in machine learning and NLP, which aligns with the job description. The applicant's interview response is in Korean, suggesting they might be comfortable communicating in this language. The CTO personas have interests in problem-solving and lessons learned, which aligns well with the software engineering role. Their communication style is rational and to the point, which is suitable for technical discussions.\n",
      "\n",
      "Final Answer: 34215d83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:43:15 [INFO] httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "router target='llm'\n",
      "{'query': '떡볶이 먹고싶다 서울 떡볶이 맛집 좀 알려줘줘', 'input_type': 'other', 'persona_id': \"Thought: The applicant is a software engineer with experience in machine learning and NLP, which aligns with the job description. The applicant's interview response is in Korean, suggesting they might be comfortable communicating in this language. The CTO personas have interests in problem-solving and lessons learned, which aligns well with the software engineering role. Their communication style is rational and to the point, which is suitable for technical discussions.\\n\\nFinal Answer: 34215d83\", 'route_type': Route(target='llm'), 'resume': \"John Doe\\r\\nSoftware Engineer\\r\\n\\r\\nEducation:\\r\\n- Bachelor's degree in Computer Science, XYZ University\\r\\n- Master's degree in Artificial Intelligence, ABC University\\r\\n\\r\\nExperience:\\r\\n- Senior Software Engineer at Tech Corp (2020-Present)\\r\\n- Software Engineer at Startup Inc (2018-2020)\\r\\n\\r\\nSkills:\\r\\n- Python, JavaScript, React\\r\\n- Machine Learning, NLP\\r\\n- Web Development\\r\\n- Cloud Computing \", 'jd': \"Software Engineer Position\\r\\n\\r\\nRequirements:\\r\\n- Bachelor's degree in Computer Science or related field\\r\\n- 3+ years of experience in software development\\r\\n- Strong knowledge of Python and web frameworks\\r\\n- Experience with machine learning and NLP is a plus\\r\\n\\r\\nResponsibilities:\\r\\n- Develop and maintain web applications\\r\\n- Implement new features and improvements\\r\\n- Collaborate with team members\\r\\n- Write clean and maintainable code \", 'company': 'Tech Corp\\r\\n\\r\\nAbout Us:\\r\\nTech Corp is a leading technology company specializing in artificial intelligence and machine learning solutions. We are committed to innovation and excellence in everything we do.\\r\\n\\r\\nCulture:\\r\\n- Collaborative and inclusive work environment\\r\\n- Emphasis on continuous learning and growth\\r\\n- Work-life balance\\r\\n- Remote-friendly workplace\\r\\n\\r\\nBenefits:\\r\\n- Competitive salary\\r\\n- Health insurance\\r\\n- 401(k) matching\\r\\n- Flexible work hours\\r\\n- Professional development opportunities\\nTech Corp\\r\\n\\r\\nAbout Us:\\r\\nTech Corp is a leading technology company specializing in artificial intelligence and machine learning solutions. We are committed to innovation and excellence in everything we do.\\r\\n\\r\\nCulture:\\r\\n- Collaborative and inclusive work environment\\r\\n- Emphasis on continuous learning and growth\\r\\n- Work-life balance\\r\\n- Remote-friendly workplace\\r\\n\\r\\nBenefits:\\r\\n- Competitive salary\\r\\n- Health insurance\\r\\n- 401(k) matching\\r\\n- Flexible work hours\\r\\n- Professional development opportunities\\nTech Corp\\r\\n\\r\\nAbout Us:\\r\\nTech Corp is a leading technology company specializing in artificial intelligence and machine learning solutions. We are committed to innovation and excellence in everything we do.\\r\\n\\r\\nCulture:\\r\\n- Collaborative and inclusive work environment\\r\\n- Emphasis on continuous learning and growth\\r\\n- Work-life balance\\r\\n- Remote-friendly workplace\\r\\n\\r\\nBenefits:\\r\\n- Competitive salary\\r\\n- Health insurance\\r\\n- 401(k) matching\\r\\n- Flexible work hours\\r\\n- Professional development opportunities', 'chat_history': '', 'last_question': None}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m initial_state = { \n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m떡볶이 먹고싶다 서울 떡볶이 맛집 좀 알려줘줘\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m: stored_resume, \n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     ) \n\u001b[32m     10\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langgraph\\pregel\\runner.py:161\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    159\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mcall_llm\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     11\u001b[39m query = state[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m llm_chain = llm | StrOutputParser()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m llm_answer = \u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m: llm_answer }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:371\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    361\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    366\u001b[39m     **kwargs: Any,\n\u001b[32m    367\u001b[39m ) -> BaseMessage:\n\u001b[32m    368\u001b[39m     config = ensure_config(config)\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    370\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    381\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:956\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    947\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    949\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    953\u001b[39m     **kwargs: Any,\n\u001b[32m    954\u001b[39m ) -> LLMResult:\n\u001b[32m    955\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:775\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    774\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m         )\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    783\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1021\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1019\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:973\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    967\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    975\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nnn15\\anaconda3\\envs\\llm\\Lib\\ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "initial_state = { \n",
    "    \"query\": \"떡볶이 먹고싶다 서울 떡볶이 맛집 좀 알려줘줘\", \n",
    "    \"resume\": stored_resume, \n",
    "    \"jd\": stored_jd,\n",
    "    \"company\": stored_company_info,\n",
    "    \"chat_history\": chat_history.get_all_history_as_string(),\n",
    "    \"last_question\": chat_history.get_question_by_id(\n",
    "        chat_history.get_latest_question_id()\n",
    "    ) \n",
    "}\n",
    "graph.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270163b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d73f8d6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
