from pydantic import BaseModel
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    BaseMessage,
    ToolMessage,
)
from langchain_core.tools import tool
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from typing import Literal
import os
import logging
import json


from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()
# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@tool
def classify_input(input):
    """ì…ë ¥ì´ ìì†Œì„œì¸ì§€, ë©´ì ‘ë‹µë³€ì¸ì§€, ì¼ë°˜ í…ìŠ¤íŠ¸ì¸ì§€ êµ¬ë¶„í•©ë‹ˆë‹¤."""
    classify_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒ ì…ë ¥ì´ ì–´ë–¤ ìœ í˜•ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”: 
        - ìì†Œì„œ (resume)
        - ë©´ì ‘ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (interview_answer)
        - ê·¸ ì™¸ ì¼ë°˜ í…ìŠ¤íŠ¸ (other)

        ì…ë ¥:
        {input}

        í˜•ì‹: resume, interview_answer, other ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•˜ì„¸ìš”.
        """
    )
    chain = classify_prompt | llm | StrOutputParser()
    return chain.invoke({"input": input})


@tool
def generate_question_reasoning(data):
    """Resumes, Job Descriptions, and Company Infoë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ì´ìœ (Reasoning)ë¥¼ ë„ì¶œ"""

    data = json.loads(data)
    resume = data.get("resume", "")
    jd = data.get("jd", "")
    company = data.get("company", "")
    # chat_history = data["chat_history"]
    persona = data.get("persona", "")

    reasoning_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒì€ í•œ ì§€ì›ìì˜ ìì†Œì„œ, JD(ì§ë¬´ê¸°ìˆ ì„œ), íšŒì‚¬ ì •ë³´ì…ë‹ˆë‹¤:

        ìì†Œì„œ:
        {resume}

        JD:
        {jd}

        íšŒì‚¬ ì •ë³´:
        {company}

        ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ë©´ì ‘ ì§ˆë¬¸ì„ ë§Œë“¤ê¸° ìœ„í•œ Reasoningì„ ì‘ì„±í•˜ì„¸ìš”:
        1. íšŒì‚¬ ì¸ì¬ìƒì— ë¶€í•©í•˜ëŠ” ì„±ê²©/ì—­ëŸ‰/í–‰ë™ì„ ìì†Œì„œì—ì„œ ì–¼ë§ˆë‚˜ í™•ì¸í•  ìˆ˜ ìˆëŠ”ê°€?
        2. JDì—ì„œ ìš”êµ¬í•˜ëŠ” ìê²©ìš”ê±´, ê¸°ìˆ , ê²½í—˜ê³¼ ìì†Œì„œê°€ ì˜ ë¶€í•©í•˜ëŠ”ê°€?
        3. ë¶€ì¡±í•˜ê±°ë‚˜ í™•ì¸ì´ í•„ìš”í•œ ì ì€ ë¬´ì—‡ì¸ê°€?

        [ì¶œë ¥ ì˜ˆì‹œ]
        - í˜‘ì—… ì—­ëŸ‰ì´ íšŒì‚¬ ì¸ì¬ìƒì—ì„œ ì¤‘ìš”í•˜ì§€ë§Œ ìì†Œì„œì— êµ¬ì²´ì ì¸ í˜‘ì—… ê²½í—˜ì´ ì–¸ê¸‰ë˜ì–´ ìˆì§€ ì•ŠìŒ.
        - JDì—ì„œ ê°•ì¡°í•œ ë°ì´í„° ë¶„ì„ ê²½í—˜ì´ ìì†Œì„œì— ì¼ë¶€ ì¡´ì¬í•˜ë‚˜ í”„ë¡œì íŠ¸ êµ¬ì²´ì„± ë¶€ì¡±.
        """
    )

    chain = reasoning_prompt | llm | StrOutputParser()
    return chain.invoke(
        {"resume": resume, "jd": jd, "company": company, "persona": persona}
    )


@tool
def generate_question_acting(reasoning):
    """Reasoningì— ê¸°ë°˜í•˜ì—¬ ì‹¤ì œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±"""
    acting_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒì€ ë©´ì ‘ ì§ˆë¬¸ì„ ë§Œë“¤ê¸° ìœ„í•œ Reasoning ë¦¬ìŠ¤íŠ¸ ì…ë‹ˆë‹¤:

        {reasoning}

        ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì ì ˆí•œ ë©´ì ‘ ì§ˆë¬¸ 1ê°œë¥¼ ì¶œë ¥í•˜ì„¸ìš”.
        
        [ì˜ˆì‹œ ì¶œë ¥]
        í˜‘ì—… ê²½í—˜ ì¤‘ ê°€ì¥ ë„ì „ì ì´ì—ˆë˜ ìƒí™©ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?
        """
    )

    chain = acting_prompt | llm | StrOutputParser()
    return chain.invoke({"reasoning": reasoning})


@tool
def generate_followup_reasoning(input_text):
    """ì§€ì›ìì˜ ë‹µë³€ì— ëŒ€í•œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•´ ì§€ì›ì ë‹µë³€ ë° ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ ì´ìœ (Reasoning) ë„ì¶œ"""

    reasoning_prompt = PromptTemplate.from_template(
        """
        ì•„ë˜ëŠ” AI ë©´ì ‘ ì‹œìŠ¤í…œì—ì„œ ì§€ê¸ˆê¹Œì§€ ì§„í–‰ëœ ì§ˆë¬¸ê³¼ ì§€ì›ìì˜ ë‹µë³€ì…ë‹ˆë‹¤:

        ëŒ€í™” ì´ë ¥:
        {chat_history}

        í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì›ìì˜ ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
        "{input_text}"

        ì´ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê¼¬ë¦¬ì§ˆë¬¸ì„ ì´ì–´ê°€ë ¤ëŠ” ë©´ì ‘ê´€ì˜ Reasoningì„ ì‘ì„±í•˜ì„¸ìš”.
        ì˜ˆë¥¼ ë“¤ì–´, ì§€ì›ìì˜ ë§ ì¤‘ êµ¬ì²´ì ì´ì§€ ì•Šì€ ë¶€ë¶„ì„ ì§šê±°ë‚˜, ê²½í—˜ì˜ ì§„ì •ì„±, ì¶”ê°€ ì„¤ëª…ì´ í•„ìš”í•œ í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ë‚´ì„¸ìš”.
        """
    )

    chain = reasoning_prompt | llm | StrOutputParser()
    return chain.invoke({})


@tool
def generate_followup_acting(reasoning, input_text):
    """Reasoningê³¼ ì‚¬ìš©ìì˜ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±"""

    prompt = PromptTemplate.from_template(
        """
        ì•„ë˜ëŠ” ì§€ì›ìì˜ ë‹µë³€ê³¼ ê·¸ì— ëŒ€í•œ ë©´ì ‘ê´€ì˜ ì¶”ë¡ (Reasoning)ì…ë‹ˆë‹¤.

        [ì§€ì›ì ë‹µë³€]
        {input_text}

        [ë©´ì ‘ê´€ì˜ Reasoning]
        {reasoning}

        ìœ„ Reasoningì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê¼¬ë¦¬ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.
        ì§ˆë¬¸ì€ ë©´ì ‘ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ìì—°ìŠ¤ëŸ½ê³  êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”.
        í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        """
    )

    chain = prompt | llm | StrOutputParser()

    return chain.invoke({"reasoning": reasoning, "input_text": input_text})


class AssessmentResult(BaseModel):
    logicScore: int
    jobFitScore: int
    coreValueFitScore: int
    communicationScore: int
    averageScore: float
@tool
def evaluate_answer(data):
    """ì§€ì›ì ë‹µë³€ì„ í‰ê°€"""


    parser = JsonOutputParser(pydantic_object=AssessmentResult)

    data = json.loads(data)
    resume = data.get("resume", "")
    jd = data.get("jd", "")
    company = data.get("company", "")
    question = data.get("question", "")
    answer = data.get("answer", "")
    persona = data.get("persona", "")

    assessment_prompt = PromptTemplate(
        input_variables=["resume", "jd", "company", "chat_history"],
        template="""
            ì—­í• :
            ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ë“£ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ì§€ì›ìê°€ ë‹µë³€í•œ ë‚´ìš©ì´ ì²˜ìŒ ì§ˆë¬¸ì— ì í•©í•œ ë‹µë³€ì¸ì§€ ì² ì €í•˜ê²Œ íŒë‹¨í•˜ê³ , ê·¸ íŒë‹¨ì— ì˜ê±°í•´ì„œ í›„ì†ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

            ìƒí™©:
            ë‹¹ì‹ ì´ ë³´ê³ ìˆëŠ” í™”ë©´ì—ëŠ” ì§€ì›ìê°€ ì§€ì›í•œ ì§ë¬´ JD, ì§€ì›ìì˜ RESUME ë“±ì´ ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤.
            ì´ ë©´ì ‘ìê°€ í•´ë‹¹ ì§ë¬´ì˜ ë‹´ë‹¹ìë¡œ ì…ì‚¬í•´ì„œ ì¶©ë¶„í•œ ì—­í• ì„ í•˜ê³ , íšŒì‚¬ì™€ í•¨ê»˜ ì„±ì¥í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ ì§€ì›ìë¥¼ ê²€ì¦í•´ì•¼í•©ë‹ˆë‹¤.

            ì§ë¬´ ì„¤ëª…:
            {jd}

            ì´ë ¥ì„œ:
            {resume}

            íšŒì‚¬ ìë£Œ:
            {company}

            ì´ì „ ì§ˆë¬¸: 
            {question}
            
            ì§€ì›ìì˜ ë‹µë³€:
            {answer}

            ë©´ì ‘ê´€ ì •ë³´:
            {persona}
            
            ìœ„ ì§ˆë¬¸/ë‹µë³€ ë‚´ìš©ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ 
            í˜„ì¬ ì§€ì›ìê°€ ë‹¤ìŒ í•­ëª©ë“¤ì— ëŒ€í•˜ì—¬ ì–¼ë§ˆë‚˜ ì˜ ë‹µë³€í–ˆëŠ”ì§€ í‰ê°€í•´ì•¼ í•œë‹¤.
            í‰ê°€ ê²°ê³¼ëŠ” 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‘œí˜„í•´ì•¼ í•œë‹¤.
            í‰ê°€ ê²°ê³¼ëŠ” ë…¼ë¦¬ì„±, ì§ë¬´ì í•©ì„±, í•µì‹¬ê°€ì¹˜ ë¶€í•©ì„±, ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥ 4ê°€ì§€ í•­ëª©ì— ëŒ€í•˜ì—¬ í‰ê°€í•´ì•¼ í•œë‹¤.
            í‰ê°€ ê²°ê³¼ëŠ” í‰ê·  ì ìˆ˜ë„ í•¨ê»˜ í‘œí˜„í•´ì•¼ í•œë‹¤.


            í‰ê°€ ê²°ê³¼ëŠ” JSON í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•´ì•¼ í•œë‹¤.
            ê²°ê³¼ ì˜ˆì‹œ : {{
                "logicScore": 7,
                "jobFitScore": 6,
                "coreValueFitScore": 3,
                "communicationScore": 7,
                "averageScore": 5.6,
            }}
        """,
    )

    chain = assessment_prompt | llm | parser

    return chain.invoke(
        {
            "jd": jd,
            "resume": resume,
            "company": company,
            "question": question,
            "answer": answer,
            "persona": persona,
        }
    )


@tool
def translate_to_korean(text: str) -> str:
    """ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤."""
    translate_prompt = PromptTemplate.from_template(
        """
    ë‹¤ìŒ ì˜ì–´ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.

    ì˜ì–´:
    {text}

    í•œêµ­ì–´:
    """
    )
    chain = translate_prompt | llm | StrOutputParser()
    return chain.invoke({"text": text})


tools = [
    classify_input,
    generate_question_reasoning,
    generate_question_acting,
    translate_to_korean,
]


def parse_role_from_message(
    message: BaseMessage,
) -> Literal["assistant", "human", "system", "tool", "unknown"]:
    """Extract role from a message"""
    if isinstance(message, AIMessage):
        return "assistant"
    elif isinstance(message, HumanMessage):
        return "human"
    elif isinstance(message, SystemMessage):
        return "system"
    elif isinstance(message, ToolMessage):
        return "tool"
    else:
        return "unknown"


# agent = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)
prompt = PromptTemplate.from_template(
    """
Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {input}
Thought: {agent_scratchpad}
"""
)

# LLM ì´ˆê¸°í™”
logger.info("Starting interview chain initialization...")
llm = ChatOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"), temperature=0.7, model_name="gpt-4o-mini"
)

# memory = MemorySaver()
agent = create_react_agent(llm, tools, prompt)

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)


def run_interview_question_pipeline(resume: str, jd: str, company: str) -> str:
    return agent_executor.invoke(
        f"generate_interview_question(resume='{resume}', jd='{jd}', company='{company}')"
    )


def get_interview_chain():
    interview_prompt = PromptTemplate(
        input_variables=["resume", "jd", "company_infos"],
        template="""
            ì—­í• :
            ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ë“£ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ì§€ì›ìê°€ ë‹µë³€í•œ ë‚´ìš©ì´ ì²˜ìŒ ì§ˆì˜ì— ì í•©í•œ ë‹µë³€ì¸ì§€ ì² ì €í•˜ê²Œ íŒë‹¨í•˜ê³ , ê·¸ íŒë‹¨ì— ì˜ê±°í•´ì„œ í›„ì†ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

            ìƒí™©:
            ë‹¹ì‹ ì´ ë³´ê³ ìˆëŠ” í™”ë©´ì—ëŠ” ì§€ì›ìê°€ ì§€ì›í•œ ì§ë¬´ JD, ì§€ì›ìì˜ RESUME ë“±ì´ ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤.
            ì´ ë©´ì ‘ìê°€ í•´ë‹¹ ì§ë¬´ì˜ ë‹´ë‹¹ìë¡œ ì…ì‚¬í•´ì„œ ì¶©ë¶„í•œ ì—­í• ì„ í•˜ê³ , íšŒì‚¬ì™€ í•¨ê»˜ ì„±ì¥í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ ì§€ì›ìë¥¼ ê²€ì¦í•´ì•¼í•©ë‹ˆë‹¤.
            

            íšŒì‚¬ ìë£Œ:
            {company_infos}

            chain of thought:
            ì§€ì›ìë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ ì´ë ¥ì„œ,JDë¥¼ íŒŒì•…í•´ì„œ ì§ˆë¬¸ì„ ìƒì„±/ì‘ë‹µ í‰ê°€ë¥¼ í•´ì•¼í•œë‹¤.
            ì•„ë˜ ì œì‹œë˜ëŠ” ì´ë ¥ì„œ, ì§ë¬´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë©´ì ‘ì§ˆë¬¸ì„ ì´ì „ì— ì œì‹œí•˜ì˜€ê³ , ë‹¹ì‹ ì€ ì´ ë”¥ë³€ì„ ë“£ê³  ì´ ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ followupí•˜ëŠ” ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•œë‹¤.
            ê¼¬ë¦¬ì§ˆë¬¸ì€ ì´ ê¸°ì—…ì´ ì§€ì›ìì—ê²Œ ê²€ì¦í•˜ê³ ì í•˜ëŠ” í•µì‹¬ì ì¸ í¬ì¸íŠ¸ë¥¼ íŒŒì•…í•˜ê¸°ìœ„í•œ ëª©ì ì´ë‹¤.
            ê¼¬ë¦¬ì§ˆë¬¸ì€ ë°˜ë“œì‹œ ì§€ì›ìì˜ ë‹µë³€ì— ëŒ€í•œ í›„ì†ì§ˆë¬¸ì´ì–´ì•¼ í•œë‹¤.


            ì´ë ¥ì„œ:
            {resume}

            ì§ë¬´ ì„¤ëª…:
            {jd}
            
            ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ ìƒì„±í•œë‹¤:
            """,
    )

    interview_chain = LLMChain(llm=llm, prompt=interview_prompt, output_key="result")
    return interview_chain


def get_followup_chain():
    followup_prompt = PromptTemplate(
        input_variables=["resume", "jd", "company_infos", "question", "user_answer"],
        template="""
            ì—­í• :
            ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ë“£ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ì§€ì›ìê°€ ë‹µë³€í•œ ë‚´ìš©ì´ ì²˜ìŒ ì§ˆì˜ì— ì í•©í•œ ë‹µë³€ì¸ì§€ ì² ì €í•˜ê²Œ íŒë‹¨í•˜ê³ , ê·¸ íŒë‹¨ì— ì˜ê±°í•´ì„œ í›„ì†ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

            ìƒí™©:
            ë‹¹ì‹ ì´ ë³´ê³ ìˆëŠ” í™”ë©´ì—ëŠ” ì§€ì›ìê°€ ì§€ì›í•œ ì§ë¬´ JD, ì§€ì›ìì˜ RESUME ë“±ì´ ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤.
            ì´ ë©´ì ‘ìê°€ í•´ë‹¹ ì§ë¬´ì˜ ë‹´ë‹¹ìë¡œ ì…ì‚¬í•´ì„œ ì¶©ë¶„í•œ ì—­í• ì„ í•˜ê³ , íšŒì‚¬ì™€ í•¨ê»˜ ì„±ì¥í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ ì§€ì›ìë¥¼ ê²€ì¦í•´ì•¼í•©ë‹ˆë‹¤.
            

            íšŒì‚¬ ìë£Œ:
            {company_infos}

            chain of thought:
            ì§€ì›ìë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ ì´ë ¥ì„œ,JDë¥¼ íŒŒì•…í•´ì„œ ì§ˆë¬¸ì„ ìƒì„±/ì‘ë‹µ í‰ê°€ë¥¼ í•´ì•¼í•œë‹¤.
            ì•„ë˜ ì œì‹œë˜ëŠ” ì´ë ¥ì„œ, ì§ë¬´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë©´ì ‘ì§ˆë¬¸ì„ ì´ì „ì— ì œì‹œí•˜ì˜€ê³ , ë‹¹ì‹ ì€ ì´ ë”¥ë³€ì„ ë“£ê³  ì´ ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ followupí•˜ëŠ” ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•œë‹¤.
            ê¼¬ë¦¬ì§ˆë¬¸ì€ ì´ ê¸°ì—…ì´ ì§€ì›ìì—ê²Œ ê²€ì¦í•˜ê³ ì í•˜ëŠ” í•µì‹¬ì ì¸ í¬ì¸íŠ¸ë¥¼ íŒŒì•…í•˜ê¸°ìœ„í•œ ëª©ì ì´ë‹¤.
            ê¼¬ë¦¬ì§ˆë¬¸ì€ ë°˜ë“œì‹œ ì§€ì›ìì˜ ë‹µë³€ì— ëŒ€í•œ í›„ì†ì§ˆë¬¸ì´ì–´ì•¼ í•œë‹¤.


            ì´ë ¥ì„œ:
            {resume}

            ì§ë¬´ ì„¤ëª…:
            {jd}
            
            ì´ì „ ì§ˆë¬¸/ë‹µë³€ ìŒë“¤:
            {prev_question_answer_pairs}

            ë‹µë³€ì€ ì§€ì›ìê°€ ì´ì „ì— í–ˆë˜ ë‹µë³€ì„ ìš”ì•½í•˜ê³ , ê·¸ ìš”ì•½ ì´í›„ í›„ì†ì§ˆë¬¸ì„ ìƒì„±í•œë‹¤.
            """,
    )

    followup_chain = LLMChain(llm=llm, prompt=followup_prompt, output_key="result")
    return followup_chain


def get_evaluate_chain():
    evaluate_prompt = PromptTemplate(
        input_variables=["resume", "jd", "company_infos", "prev_question_answer_pairs"],
        template="""
            ì—­í• :
            ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ë“£ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ì§€ì›ìê°€ ë‹µë³€í•œ ë‚´ìš©ì´ ì²˜ìŒ ì§ˆë¬¸ì— ì í•©í•œ ë‹µë³€ì¸ì§€ ì² ì €í•˜ê²Œ íŒë‹¨í•˜ê³ , ê·¸ íŒë‹¨ì— ì˜ê±°í•´ì„œ í›„ì†ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

            ìƒí™©:
            ë‹¹ì‹ ì´ ë³´ê³ ìˆëŠ” í™”ë©´ì—ëŠ” ì§€ì›ìê°€ ì§€ì›í•œ ì§ë¬´ JD, ì§€ì›ìì˜ RESUME ë“±ì´ ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤.
            ì´ ë©´ì ‘ìê°€ í•´ë‹¹ ì§ë¬´ì˜ ë‹´ë‹¹ìë¡œ ì…ì‚¬í•´ì„œ ì¶©ë¶„í•œ ì—­í• ì„ í•˜ê³ , íšŒì‚¬ì™€ í•¨ê»˜ ì„±ì¥í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ ì§€ì›ìë¥¼ ê²€ì¦í•´ì•¼í•©ë‹ˆë‹¤.

            ì§ë¬´ ì„¤ëª…:
            {jd}

            ì´ë ¥ì„œ:
            {resume}

            íšŒì‚¬ ìë£Œ:
            {company_infos}

            ì´ì „ ì§ˆë¬¸/ë‹µë³€ : 
            {prev_question_answer_pairs}
            
            ë‹µë³€ í‰ê°€:
            ë‹µë³€ì´ ì²˜ìŒ ì§ˆë¬¸ì— ì í•©í•œ ë‹µë³€ì¸ì§€ ì² ì €í•˜ê²Œ íŒë‹¨í•´ì•¼ í•œë‹¤.
        """,
    )

    evaluate_chain = LLMChain(llm=llm, prompt=evaluate_prompt, output_key="result")
    return evaluate_chain


def get_model_answer_chain():
    model_answer_prompt = PromptTemplate(
        input_variables=["resume", "jd", "company_infos", "question"],
        template="""
            ì—­í• :
            ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ë“£ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ì§€ì›ìê°€ ë‹µë³€í•œ ë‚´ìš©ì´ ì²˜ìŒ ì§ˆë¬¸ì— ì í•©í•œ ë‹µë³€ì¸ì§€ ì² ì €í•˜ê²Œ íŒë‹¨í•˜ê³ , ê·¸ íŒë‹¨ì— ì˜ê±°í•´ì„œ í›„ì†ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

            ìƒí™©:
            ë‹¹ì‹ ì´ ë³´ê³ ìˆëŠ” í™”ë©´ì—ëŠ” ì§€ì›ìê°€ ì§€ì›í•œ ì§ë¬´ JD, ì§€ì›ìì˜ RESUME ë“±ì´ ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤.
            ì´ ë©´ì ‘ìê°€ í•´ë‹¹ ì§ë¬´ì˜ ë‹´ë‹¹ìë¡œ ì…ì‚¬í•´ì„œ ì¶©ë¶„í•œ ì—­í• ì„ í•˜ê³ , íšŒì‚¬ì™€ í•¨ê»˜ ì„±ì¥í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ ì§€ì›ìë¥¼ ê²€ì¦í•´ì•¼í•©ë‹ˆë‹¤.

            íšŒì‚¬ ìë£Œ:
            {company_infos}

            ì§ë¬´ ì„¤ëª…:
            {jd}

            ì´ë ¥ì„œ:
            {resume}

            ì§ˆë¬¸:
            {question}

            ëª¨ë¸ ë‹µë³€:
            ìœ„ ìƒí™©ì—ì„œ ì§€ì›ìê°€ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì„ ì˜ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•œë‹¤.
            ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:

            1. STAR ê¸°ë²•ì„ í™œìš©í•œ êµ¬ì¡°í™”ëœ ë‹µë³€:
               - Situation: ìƒí™© ì„¤ëª…
               - Task: í•´ê²°í•´ì•¼ í•  ê³¼ì œ
               - Action: ì·¨í•œ í–‰ë™
               - Result: ê²°ê³¼ì™€ ë°°ìš´ ì 

            2. ì§ë¬´ ê´€ë ¨ì„±:
               - JDì—ì„œ ìš”êµ¬í•˜ëŠ” ì—­ëŸ‰ê³¼ì˜ ì—°ê´€ì„±
               - íšŒì‚¬ì˜ í•µì‹¬ ê°€ì¹˜ì™€ì˜ ë¶€í•©ì„±

            3. êµ¬ì²´ì„±:
               - êµ¬ì²´ì ì¸ ìˆ«ìì™€ ë°ì´í„° í¬í•¨
               - ì‹¤ì œ ê²½í—˜ ê¸°ë°˜ì˜ ì˜ˆì‹œ

            4. ë…¼ë¦¬ì„±:
               - ëª…í™•í•œ ì¸ê³¼ê´€ê³„
               - ì²´ê³„ì ì¸ ì„¤ëª…

            ë‹µë³€ì€ í•œê¸€ë¡œ ìƒì„±í•´ì•¼ í•œë‹¤.
        """,
    )

    model_answer_chain = LLMChain(
        llm=llm, prompt=model_answer_prompt, output_key="result"
    )
    return model_answer_chain


def get_initial_message_chain():
    initial_prompt = PromptTemplate(
        template="""
        ë‹¹ì‹ ì€ AI ë©´ì ‘ ì‹œë®¬ë ˆì´í„°ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë©´ì ‘ì„ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì•ˆë‚´í•˜ëŠ” ì²« ë©”ì‹œì§€ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        ì¡°ê±´:
        - ì‚¬ìš©ìê°€ ë‹¹ì‹ ì´ AI ë©´ì ‘ê´€ì„ì„ ì´í•´í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        - ì‚¬ìš©ìê°€ ìì†Œì„œ(ì´ë ¥ì„œ)ë¥¼ ë¨¼ì € ì…ë ¥í•´ì•¼ í•œë‹¤ëŠ” ì ì„ ê¼­ ì„¤ëª…í•˜ì„¸ìš”.
        - ë„ˆë¬´ ë”±ë”±í•˜ê±°ë‚˜ ê¸°ê³„ì ì´ì§€ ì•Šê³ , ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ëŠ” ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        - í•œ ë¬¸ë‹¨(2~4ì¤„) ë¶„ëŸ‰ì˜ ì§§ì€ ì¸ì‚¿ë§ë¡œ ì‘ì„±í•˜ê³ , ì§§ì€ í˜¸í¡ìœ¼ë¡œ ì¤„ë°”ê¿ˆ(\n)í•˜ì„¸ìš”.

        ì˜ˆì‹œ1:
        ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì—¬ëŸ¬ë¶„ì˜ ë©´ì ‘ì„ ë„ì™€ë“œë¦´ AI ì‹œë®¬ë ˆì´í„°ì…ë‹ˆë‹¤. ğŸ‘‹
        ì €ëŠ” ìì†Œì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆìƒ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³ , 
        ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        ë©´ì ‘ì„ ì‹œì‘í•˜ë ¤ë©´ ë¨¼ì € ë³¸ì¸ì˜ ì´ë ¥ì„œ(ìê¸°ì†Œê°œì„œ)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.
        ì…ë ¥í•˜ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©´ì ‘ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤.

        ì˜ˆì‹œ2:
        AI ë©´ì ‘ ì‹œë®¬ë ˆì´í„°ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤ :) 
        ë¨¼ì € ìì†Œì„œ(ì´ë ¥ì„œ)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.
        í•´ë‹¹ ë‚´ìš©ì„ ë¶„ì„í•´ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©´ì ‘ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤.

        ì˜ˆì‹œ3:
        ë°˜ê°‘ìŠµë‹ˆë‹¤! ì§€ê¸ˆë¶€í„° AIê°€ ë©´ì ‘ê´€ ì—­í• ì„ ëŒ€ì‹ í•´ë“œë¦´ê²Œìš”. 
        ì‹œì‘í•˜ë ¤ë©´ ë³¸ì¸ì˜ ì´ë ¥ì„œ(ìê¸°ì†Œê°œì„œ)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.
        ì…ë ¥í•˜ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©´ì ‘ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤. ğŸ”¥
        """
    )
    initial_chain = LLMChain(llm=llm, prompt=initial_prompt, output_key="result")
    return initial_chain


def get_reranking_model_answer_chain():
    reranking_prompt = PromptTemplate(
        input_variables=[
            "resume",
            "jd",
            "company_infos",
            "question",
            "prev_question_answer_pairs",
        ],
        template="""
            ì—­í• :
            ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ì´ ë‹µë³€ì€ íšŒì‚¬ì˜ ê°€ì¹˜ê´€, ì§ë¬´ ìš”êµ¬ì‚¬í•­, ê·¸ë¦¬ê³  ì´ë ¥ì„œì˜ ë‚´ìš©ì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

            ìƒí™©:
            {company_infos}

            ì´ë ¥ì„œ:
            {resume}

            ì§ë¬´ ì„¤ëª…:
            {jd}

            ì´ì „ ì§ˆë¬¸/ë‹µë³€ ìŒë“¤:
            {prev_question_answer_pairs}

            í˜„ì¬ ì§ˆë¬¸:
            {question}

            ë‹¤ìŒ ë‹¨ê³„ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:
            1. íšŒì‚¬ì˜ ê°€ì¹˜ê´€ê³¼ ì§ë¬´ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„
            2. ì´ë ¥ì„œì—ì„œ ê´€ë ¨ëœ ê²½í—˜ê³¼ ì—­ëŸ‰ì„ ì°¾ì•„ ì—°ê²°
            3. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ êµ¬ì„±
            4. êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì˜ˆì‹œë¥¼ í¬í•¨
            5. íšŒì‚¬ì˜ ê°€ì¹˜ê´€ê³¼ ë¶€í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ë§ˆë¬´ë¦¬

            ë‹µë³€ì€ í•œê¸€ë¡œ ìƒì„±í•´ì•¼ í•œë‹¤.
            """,
    )

    reranking_chain = LLMChain(llm=llm, prompt=reranking_prompt, output_key="result")
    return reranking_chain


def compare_model_answers(original_answer: str, reranked_answer: str) -> dict:
    """ë‘ ëª¨ë¸ ë‹µë³€ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜"""
    comparison_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
ë‹¹ì‹ ì€ ë‘ ê°œì˜ ë©´ì ‘ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ì¸ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ë‘ ë‹µë³€ì„ ë¹„êµí•˜ì„¸ìš”:
1. êµ¬ì²´ì„±: ì˜ˆì‹œì™€ ê²½í—˜ì´ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œë˜ì—ˆëŠ”ê°€?
2. ê´€ë ¨ì„±: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ë¬´ ìš”êµ¬ì‚¬í•­ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ê°€?
3. êµ¬ì¡°í™”: ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ë…¼ë¦¬ì ì´ê³  ëª…í™•í•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ê°€?
4. íšŒì‚¬ ì í•©ì„±: ë‹µë³€ì´ íšŒì‚¬ì˜ ê°€ì¹˜ê´€ê³¼ ë¬¸í™”ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ê°€?
5. ì „ë¬¸ì„±: ë‹µë³€ì´ ì§ë¬´ ê´€ë ¨ ì§€ì‹ê³¼ ì—­ëŸ‰ì„ ì–¼ë§ˆë‚˜ ì˜ ë³´ì—¬ì£¼ëŠ”ê°€?

ê° ê¸°ì¤€ë³„ë¡œ
- ì›ë³¸ ë‹µë³€ê³¼ reranking ë‹µë³€ ê°ê° 1~10ì ìœ¼ë¡œ í‰ê°€
- ê°„ë‹¨í•œ ì„¤ëª…
- ì–´ë–¤ ë‹µë³€ì´ ë” ë‚˜ì€ì§€

ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì„¸ìš”(ì¶”ê°€ ì„¤ëª…, ì½”ë“œë¸”ë¡ ë“± ê¸ˆì§€):
{{
    "specificity": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "relevance": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "structure": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "company_fit": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "expertise": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "overall": {{
        "original_total": number,
        "reranked_total": number,
        "better_answer": "original" ë˜ëŠ” "reranked",
        "summary": string
    }}
}}
ëª¨ë“  ì„¤ëª…ê³¼ ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
            ),
            (
                "user",
                """ë‹¤ìŒ ë‘ ë‹µë³€ì„ ë¹„êµí•˜ì„¸ìš”:

ì›ë³¸ ë‹µë³€:
{original_answer}

Reranking ë‹µë³€:
{reranked_answer}""",
            ),
        ]
    )

    chain = comparison_prompt | llm | StrOutputParser()

    try:
        result = chain.invoke(
            {"original_answer": original_answer, "reranked_answer": reranked_answer}
        )
        # ì½”ë“œë¸”ë¡ ë“± ì œê±°
        import re

        result_str = result.strip()
        # ```json ... ``` ë˜ëŠ” ``` ... ``` ì œê±°
        result_str = re.sub(
            r"^```(?:json)?|```$", "", result_str, flags=re.MULTILINE
        ).strip()
        comparison_result = json.loads(result_str)
        return comparison_result
    except Exception as e:
        logger.error(f"Error in comparing answers: {str(e)}")
        raise Exception(f"Failed to compare answers: {str(e)}")
