from pydantic import BaseModel
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    BaseMessage,
    ToolMessage,
)
from langchain_core.tools import tool
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from typing import Literal
import os
import logging
import json

from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()
# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


@tool
def classify_input(input):
    """ì‚¬ìš©ì ì…ë ¥ê³¼, ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì…ë ¥ì´ ì–´ë–¤ í˜•ì‹ì¸ì§€ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    data = json.loads(input)
    request = data["request"]
    chat_history = data["chat_history"]

    """ì…ë ¥ì´ ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    classify_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒ ì…ë ¥ì´ ì–´ë–¤ ìœ í˜•ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”: 
        - ìì†Œì„œ ì…ë ¥ (resume)
        - ì§ˆë¬¸ ìš”ì²­ (question)
        - ê¼¬ë¦¬ì§ˆë¬¸ ìš”ì²­ (followup)
        - ëª¨ë²” ë‹µë³€ ìš”ì²­ (modelAnswer)
        - ë‹µë³€ ìš”ì²­ (answer)
        - ê·¸ ì™¸ ì¼ë°˜ í…ìŠ¤íŠ¸ (other)

        ì…ë ¥:
        ì‚¬ìš©ì ì…ë ¥ : {request}
        ì´ì „ ëŒ€í™” ë‚´ìš© : {chat_history}

        í˜•ì‹: question, followup, modelAnswer, answer, other ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•˜ì„¸ìš”.
        """
    )
    chain = classify_prompt | llm | StrOutputParser()
    return chain.invoke({"request": request, "chat_history": chat_history})


@tool
def generate_question_reasoning(data):
    """Resumes, Job Descriptions, and Company Infoë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ì´ìœ (Reasoning)ë¥¼ ë„ì¶œ"""

    data = json.loads(data)
    resume = data.get("resume", "")
    jd = data.get("jd", "")
    company = data.get("company", "")
    # chat_history = data["chat_history"]
    persona = data.get("persona", "")

    reasoning_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒì€ ì§€ì›ìì˜ ìì†Œì„œ, JD(ì§ë¬´ê¸°ìˆ ì„œ), íšŒì‚¬ ì •ë³´, ì´ì „ ëŒ€í™” ì´ë ¥, ê·¸ë¦¬ê³  ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ì…ë‹ˆë‹¤:

        ìê¸°ì†Œê°œì„œ:
        {resume}

        JD:
        {jd}

        íšŒì‚¬ ì •ë³´:
        {company}

        ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ (ì„±ê²©, ê´€ì‹¬ì‚¬, ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤íƒ€ì¼ ë“±):
        {persona}

        ---

        ë‹¹ì‹ ì€ ìœ„ í˜ë¥´ì†Œë‚˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
        ì¦‰, ì§ˆë¬¸ì„ ë§Œë“œëŠ” ê³¼ì •ì—ì„œ ë‹¤ìŒì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤:

        - ì´ ë©´ì ‘ê´€ì€ ì–´ë–¤ ê´€ì ì—ì„œ ì§€ì›ìë¥¼ ë°”ë¼ë³¼ê¹Œ?
        - ì–´ë–¤ ì—­ëŸ‰ì´ë‚˜ í‚¤ì›Œë“œì— íŠ¹íˆ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í• ê¹Œ?
        - ì§ˆë¬¸ì„ ë˜ì§ˆ ë•Œ ì–´ë–¤ ìŠ¤íƒ€ì¼(ë…¼ë¦¬ì , ë‚ ì¹´ë¡œìš´, ë”°ëœ»í•œ ë“±)ë¡œ Reasoningì„ ì „ê°œí• ê¹Œ?

        ì•„ë˜ ë‚´ìš©ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ Reasoningì„ ì‘ì„±í•˜ì„¸ìš”:

        1. íšŒì‚¬ ì¸ì¬ìƒì— ë¶€í•©í•˜ëŠ” ì„±ê²©/ì—­ëŸ‰/í–‰ë™ì„ ìì†Œì„œì—ì„œ ì–¼ë§ˆë‚˜ í™•ì¸í•  ìˆ˜ ìˆëŠ”ê°€?
        2. JDì—ì„œ ìš”êµ¬í•˜ëŠ” ìê²©ìš”ê±´, ê¸°ìˆ , ê²½í—˜ê³¼ ìì†Œì„œê°€ ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ê°€?
        3. ë¶€ì¡±í•˜ê±°ë‚˜ í™•ì¸ì´ í•„ìš”í•œ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€?
        4. (ì¤‘ìš”) ì´ ë‚´ìš©ì„ ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ì˜ ì‹œê°ê³¼ ë§íˆ¬, ì„±ê²©ì„ ë°˜ì˜í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.

        ì¶œë ¥ ì˜ˆì‹œ:
        - (ë…¼ë¦¬ì  ìŠ¤íƒ€ì¼) "JDì—ì„œ ìš”êµ¬í•œ í˜‘ì—… ê²½í—˜ì´ ìì†Œì„œì— êµ¬ì²´ì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ì§€ ì•Šì•„, ì‹¤ì œë¡œ íŒ€ í”„ë¡œì íŠ¸ë¥¼ ì£¼ë„í•œ ê²½í—˜ì´ ìˆì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¶ë‹¤."
        - (í˜¸ê¸°ì‹¬ ë§ì€ ìŠ¤íƒ€ì¼) "ì§€ì›ìëŠ” ë°ì´í„° ë¶„ì„ ê²½í—˜ì´ ìˆë‹¤ê³  í–ˆì§€ë§Œ ì–´ë–¤ íˆ´ì„ ì‚¬ìš©í–ˆëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤. í”„ë¡œì íŠ¸ ë§¥ë½ì„ ë” ë“£ê³  ì‹¶ë‹¤."
        """
    )

    chain = reasoning_prompt | llm | StrOutputParser()
    return chain.invoke(
        {"resume": resume, "jd": jd, "company": company, "persona": persona}
    )


@tool
def generate_question_acting(reasoning):
    """Reasoningì— ê¸°ë°˜í•˜ì—¬ ì‹¤ì œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±"""
    acting_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒì€ ë©´ì ‘ ì§ˆë¬¸ì„ ë§Œë“¤ê¸° ìœ„í•œ Reasoning ë¦¬ìŠ¤íŠ¸ ì…ë‹ˆë‹¤:

        {reasoning}

        ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì ì ˆí•œ ë©´ì ‘ ì§ˆë¬¸ 1ê°œë¥¼ ì¶œë ¥í•˜ì„¸ìš”.
        
        [ì˜ˆì‹œ ì¶œë ¥]
        í˜‘ì—… ê²½í—˜ ì¤‘ ê°€ì¥ ë„ì „ì ì´ì—ˆë˜ ìƒí™©ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?
        """
    )

    chain = acting_prompt | llm | StrOutputParser()
    return chain.invoke({"reasoning": reasoning})


@tool
def generate_followup_reasoning(input_text):
    """ì§€ì›ìì˜ ë‹µë³€ì— ëŒ€í•œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•´ ì§€ì›ì ë‹µë³€ ë° ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ ì´ìœ (Reasoning) ë„ì¶œ"""

    reasoning_prompt = PromptTemplate.from_template(
        """
        ì•„ë˜ëŠ” AI ë©´ì ‘ ì‹œìŠ¤í…œì—ì„œ ì§€ê¸ˆê¹Œì§€ ì§„í–‰ëœ ì§ˆë¬¸ê³¼ ì§€ì›ìì˜ ë‹µë³€ì…ë‹ˆë‹¤:

        ëŒ€í™” ì´ë ¥:
        {chat_history}

        í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì›ìì˜ ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
        "{input_text}"

        ì´ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê¼¬ë¦¬ì§ˆë¬¸ì„ ì´ì–´ê°€ë ¤ëŠ” ë©´ì ‘ê´€ì˜ Reasoningì„ ì‘ì„±í•˜ì„¸ìš”.
        ì˜ˆë¥¼ ë“¤ì–´, ì§€ì›ìì˜ ë§ ì¤‘ êµ¬ì²´ì ì´ì§€ ì•Šì€ ë¶€ë¶„ì„ ì§šê±°ë‚˜, ê²½í—˜ì˜ ì§„ì •ì„±, ì¶”ê°€ ì„¤ëª…ì´ í•„ìš”í•œ í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ë‚´ì„¸ìš”.
        """
    )

    chain = reasoning_prompt | llm | StrOutputParser()
    return chain.invoke({})


@tool
def generate_followup_acting(reasoning, input_text):
    """Reasoningê³¼ ì‚¬ìš©ìì˜ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±"""

    prompt = PromptTemplate.from_template(
        """
        ì•„ë˜ëŠ” ì§€ì›ìì˜ ë‹µë³€ê³¼ ê·¸ì— ëŒ€í•œ ë©´ì ‘ê´€ì˜ ì¶”ë¡ (Reasoning)ì…ë‹ˆë‹¤.

        [ì§€ì›ì ë‹µë³€]
        {input_text}

        [ë©´ì ‘ê´€ì˜ Reasoning]
        {reasoning}

        ìœ„ Reasoningì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê¼¬ë¦¬ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.
        ì§ˆë¬¸ì€ ë©´ì ‘ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ìì—°ìŠ¤ëŸ½ê³  êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”.
        í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        """
    )

    chain = prompt | llm | StrOutputParser()

    return chain.invoke({"reasoning": reasoning, "input_text": input_text})


class AssessmentResult(BaseModel):
    logicScore: int
    jobFitScore: int
    coreValueFitScore: int
    communicationScore: int
    averageScore: float


@tool
def evaluate_answer(data):
    """ì§€ì›ì ë‹µë³€ì„ í‰ê°€"""

    parser = JsonOutputParser(pydantic_object=AssessmentResult)

    data = json.loads(data)
    resume = data.get("resume", "")
    jd = data.get("jd", "")
    company = data.get("company", "")
    question = data.get("question", "")
    answer = data.get("answer", "")
    persona = data.get("persona", "")

    assessment_prompt = PromptTemplate(
        input_variables=["resume", "jd", "company", "chat_history"],
        template="""
            ì—­í• :
            ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ë“£ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ì§€ì›ìê°€ ë‹µë³€í•œ ë‚´ìš©ì´ ì²˜ìŒ ì§ˆë¬¸ì— ì í•©í•œ ë‹µë³€ì¸ì§€ ì² ì €í•˜ê²Œ íŒë‹¨í•˜ê³ , ê·¸ íŒë‹¨ì— ì˜ê±°í•´ì„œ í›„ì†ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

            ìƒí™©:
            ë‹¹ì‹ ì´ ë³´ê³ ìˆëŠ” í™”ë©´ì—ëŠ” ì§€ì›ìê°€ ì§€ì›í•œ ì§ë¬´ JD, ì§€ì›ìì˜ RESUME ë“±ì´ ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤.
            ì´ ë©´ì ‘ìê°€ í•´ë‹¹ ì§ë¬´ì˜ ë‹´ë‹¹ìë¡œ ì…ì‚¬í•´ì„œ ì¶©ë¶„í•œ ì—­í• ì„ í•˜ê³ , íšŒì‚¬ì™€ í•¨ê»˜ ì„±ì¥í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ ì§€ì›ìë¥¼ ê²€ì¦í•´ì•¼í•©ë‹ˆë‹¤.

            ì§ë¬´ ì„¤ëª…:
            {jd}

            ì´ë ¥ì„œ:
            {resume}

            íšŒì‚¬ ìë£Œ:
            {company}

            ì´ì „ ì§ˆë¬¸: 
            {question}
            
            ì§€ì›ìì˜ ë‹µë³€:
            {answer}

            ë©´ì ‘ê´€ ì •ë³´:
            {persona}
            
            ìœ„ ì§ˆë¬¸/ë‹µë³€ ë‚´ìš©ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ 
            í˜„ì¬ ì§€ì›ìê°€ ë‹¤ìŒ í•­ëª©ë“¤ì— ëŒ€í•˜ì—¬ ì–¼ë§ˆë‚˜ ì˜ ë‹µë³€í–ˆëŠ”ì§€ í‰ê°€í•´ì•¼ í•œë‹¤.
            í‰ê°€ ê²°ê³¼ëŠ” 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‘œí˜„í•´ì•¼ í•œë‹¤.
            í‰ê°€ ê²°ê³¼ëŠ” ë…¼ë¦¬ì„±, ì§ë¬´ì í•©ì„±, í•µì‹¬ê°€ì¹˜ ë¶€í•©ì„±, ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥ 4ê°€ì§€ í•­ëª©ì— ëŒ€í•˜ì—¬ í‰ê°€í•´ì•¼ í•œë‹¤.
            í‰ê°€ ê²°ê³¼ëŠ” í‰ê·  ì ìˆ˜ë„ í•¨ê»˜ í‘œí˜„í•´ì•¼ í•œë‹¤.


            í‰ê°€ ê²°ê³¼ëŠ” JSON í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•´ì•¼ í•œë‹¤.
            ê²°ê³¼ ì˜ˆì‹œ : {{
                "logicScore": 7,
                "jobFitScore": 6,
                "coreValueFitScore": 3,
                "communicationScore": 7,
                "averageScore": 5.6,
            }}
        """,
    )

    chain = assessment_prompt | llm | parser

    return chain.invoke(
        {
            "jd": jd,
            "resume": resume,
            "company": company,
            "question": question,
            "answer": answer,
            "persona": persona,
        }
    )


@tool
def translate_to_korean(text: str) -> str:
    """ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤."""
    translate_prompt = PromptTemplate.from_template(
        """
    ë‹¤ìŒ ì˜ì–´ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.

    ì˜ì–´:
    {text}

    í•œêµ­ì–´:
    """
    )
    chain = translate_prompt | llm | StrOutputParser()
    return chain.invoke({"text": text})


tools = [
    classify_input,
    generate_question_reasoning,
    generate_question_acting,
    translate_to_korean,
]


def parse_role_from_message(
    message: BaseMessage,
) -> Literal["assistant", "human", "system", "tool", "unknown"]:
    """Extract role from a message"""
    if isinstance(message, AIMessage):
        return "assistant"
    elif isinstance(message, HumanMessage):
        return "human"
    elif isinstance(message, SystemMessage):
        return "system"
    elif isinstance(message, ToolMessage):
        return "tool"
    else:
        return "unknown"


# agent = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)
prompt = PromptTemplate.from_template(
    """
Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {input}
Thought: {agent_scratchpad}
"""
)

# LLM ì´ˆê¸°í™”
logger.info("Starting interview chain initialization...")
llm = ChatOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    temperature=0.7,
    model_name="gpt-4o",
    verbose=True,
)

# memory = MemorySaver()
agent = create_react_agent(llm, tools, prompt)

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)


def run_interview_question_pipeline(resume: str, jd: str, company: str) -> str:
    return agent_executor.invoke(
        f"generate_interview_question(resume='{resume}', jd='{jd}', company='{company}')"
    )


def get_initial_message_chain():
    initial_prompt = PromptTemplate(
        template="""
        ë‹¹ì‹ ì€ AI ë©´ì ‘ ì‹œë®¬ë ˆì´í„°ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë©´ì ‘ì„ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì•ˆë‚´í•˜ëŠ” ì²« ë©”ì‹œì§€ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        ì¡°ê±´:
        - ì‚¬ìš©ìê°€ ë‹¹ì‹ ì´ AI ë©´ì ‘ê´€ì„ì„ ì´í•´í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        - ì‚¬ìš©ìê°€ ìì†Œì„œ(ì´ë ¥ì„œ)ë¥¼ ë¨¼ì € ì…ë ¥í•´ì•¼ í•œë‹¤ëŠ” ì ì„ ê¼­ ì„¤ëª…í•˜ì„¸ìš”.
        - ë„ˆë¬´ ë”±ë”±í•˜ê±°ë‚˜ ê¸°ê³„ì ì´ì§€ ì•Šê³ , ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ëŠ” ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        - í•œ ë¬¸ë‹¨(2~4ì¤„) ë¶„ëŸ‰ì˜ ì§§ì€ ì¸ì‚¿ë§ë¡œ ì‘ì„±í•˜ê³ , ì§§ì€ í˜¸í¡ìœ¼ë¡œ ì¤„ë°”ê¿ˆ(\n)í•˜ì„¸ìš”.

        ì˜ˆì‹œ1:
        ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì—¬ëŸ¬ë¶„ì˜ ë©´ì ‘ì„ ë„ì™€ë“œë¦´ AI ì‹œë®¬ë ˆì´í„°ì…ë‹ˆë‹¤. ğŸ‘‹
        ì €ëŠ” ìì†Œì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆìƒ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³ , 
        ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        ë©´ì ‘ì„ ì‹œì‘í•˜ë ¤ë©´ ë¨¼ì € ë³¸ì¸ì˜ ì´ë ¥ì„œ(ìê¸°ì†Œê°œì„œ)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.
        ì…ë ¥í•˜ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©´ì ‘ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤.

        ì˜ˆì‹œ2:
        AI ë©´ì ‘ ì‹œë®¬ë ˆì´í„°ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤ :) 
        ë¨¼ì € ìì†Œì„œ(ì´ë ¥ì„œ)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.
        í•´ë‹¹ ë‚´ìš©ì„ ë¶„ì„í•´ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©´ì ‘ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤.

        ì˜ˆì‹œ3:
        ë°˜ê°‘ìŠµë‹ˆë‹¤! ì§€ê¸ˆë¶€í„° AIê°€ ë©´ì ‘ê´€ ì—­í• ì„ ëŒ€ì‹ í•´ë“œë¦´ê²Œìš”. 
        ì‹œì‘í•˜ë ¤ë©´ ë³¸ì¸ì˜ ì´ë ¥ì„œ(ìê¸°ì†Œê°œì„œ)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.
        ì…ë ¥í•˜ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©´ì ‘ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤. ğŸ”¥
        """
    )
    initial_chain = LLMChain(llm=llm, prompt=initial_prompt, output_key="result")
    return initial_chain


def get_reranking_model_answer_chain():
    reranking_prompt = PromptTemplate(
        input_variables=[
            "resume",
            "jd",
            "company_infos",
            "question",
            "prev_question_answer_pairs",
        ],
        template="""
            ì—­í• :
            ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ì´ ë‹µë³€ì€ íšŒì‚¬ì˜ ê°€ì¹˜ê´€, ì§ë¬´ ìš”êµ¬ì‚¬í•­, ê·¸ë¦¬ê³  ì´ë ¥ì„œì˜ ë‚´ìš©ì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

            ìƒí™©:
            {company_infos}

            ì´ë ¥ì„œ:
            {resume}

            ì§ë¬´ ì„¤ëª…:
            {jd}

            ì´ì „ ì§ˆë¬¸/ë‹µë³€ ìŒë“¤:
            {prev_question_answer_pairs}

            í˜„ì¬ ì§ˆë¬¸:
            {question}

            ëª¨ë¸ ë‹µë³€:
            ìœ„ ìƒí™©ì—ì„œ ì§€ì›ìê°€ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì„ ì˜ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•œë‹¤.
            ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:

            1. STAR ê¸°ë²•ì„ í™œìš©í•œ êµ¬ì¡°í™”ëœ ë‹µë³€:
               - Situation: ìƒí™© ì„¤ëª…
               - Task: í•´ê²°í•´ì•¼ í•  ê³¼ì œ
               - Action: ì·¨í•œ í–‰ë™
               - Result: ê²°ê³¼ì™€ ë°°ìš´ ì 

            2. ì§ë¬´ ê´€ë ¨ì„±:
               - JDì—ì„œ ìš”êµ¬í•˜ëŠ” ì—­ëŸ‰ê³¼ì˜ ì—°ê´€ì„±
               - íšŒì‚¬ì˜ í•µì‹¬ ê°€ì¹˜ì™€ì˜ ë¶€í•©ì„±

            3. êµ¬ì²´ì„±:
               - êµ¬ì²´ì ì¸ ìˆ«ìì™€ ë°ì´í„° í¬í•¨
               - ì‹¤ì œ ê²½í—˜ ê¸°ë°˜ì˜ ì˜ˆì‹œ

            4. ë…¼ë¦¬ì„±:
               - ëª…í™•í•œ ì¸ê³¼ê´€ê³„
               - ì²´ê³„ì ì¸ ì„¤ëª…
               
            ë‹µë³€ì€ í•œê¸€ë¡œ ìƒì„±í•´ì•¼ í•œë‹¤.

            """,
    )

    reranking_chain = LLMChain(llm=llm, prompt=reranking_prompt, output_key="result")
    return reranking_chain


def compare_model_answers(original_answer: str, reranked_answer: str) -> dict:
    """ë‘ ëª¨ë¸ ë‹µë³€ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜"""
    comparison_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
ë‹¹ì‹ ì€ ë‘ ê°œì˜ ë©´ì ‘ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ì¸ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ë‘ ë‹µë³€ì„ ë¹„êµí•˜ì„¸ìš”:
1. êµ¬ì²´ì„±: ì˜ˆì‹œì™€ ê²½í—˜ì´ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œë˜ì—ˆëŠ”ê°€?
2. ê´€ë ¨ì„±: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ë¬´ ìš”êµ¬ì‚¬í•­ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ê°€?
3. êµ¬ì¡°í™”: ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ë…¼ë¦¬ì ì´ê³  ëª…í™•í•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ê°€?
4. íšŒì‚¬ ì í•©ì„±: ë‹µë³€ì´ íšŒì‚¬ì˜ ê°€ì¹˜ê´€ê³¼ ë¬¸í™”ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ê°€?
5. ì „ë¬¸ì„±: ë‹µë³€ì´ ì§ë¬´ ê´€ë ¨ ì§€ì‹ê³¼ ì—­ëŸ‰ì„ ì–¼ë§ˆë‚˜ ì˜ ë³´ì—¬ì£¼ëŠ”ê°€?

ê° ê¸°ì¤€ë³„ë¡œ
- ì›ë³¸ ë‹µë³€ê³¼ reranking ë‹µë³€ ê°ê° 1~10ì ìœ¼ë¡œ í‰ê°€
- ê°„ë‹¨í•œ ì„¤ëª…
- ì–´ë–¤ ë‹µë³€ì´ ë” ë‚˜ì€ì§€

ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì„¸ìš”(ì¶”ê°€ ì„¤ëª…, ì½”ë“œë¸”ë¡ ë“± ê¸ˆì§€):
{{
    "specificity": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "relevance": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "structure": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "company_fit": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "expertise": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "overall": {{
        "original_total": number,
        "reranked_total": number,
        "better_answer": "original" ë˜ëŠ” "reranked",
        "summary": string
    }}
}}
ëª¨ë“  ì„¤ëª…ê³¼ ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
            ),
            (
                "user",
                """ë‹¤ìŒ ë‘ ë‹µë³€ì„ ë¹„êµí•˜ì„¸ìš”:

ì›ë³¸ ë‹µë³€:
{original_answer}

Reranking ë‹µë³€:
{reranked_answer}""",
            ),
        ]
    )

    chain = comparison_prompt | llm | StrOutputParser()

    try:
        result = chain.invoke(
            {"original_answer": original_answer, "reranked_answer": reranked_answer}
        )
        # ì½”ë“œë¸”ë¡ ë“± ì œê±°
        import re

        result_str = result.strip()
        # ```json ... ``` ë˜ëŠ” ``` ... ``` ì œê±°
        result_str = re.sub(
            r"^```(?:json)?|```$", "", result_str, flags=re.MULTILINE
        ).strip()
        comparison_result = json.loads(result_str)
        return comparison_result
    except Exception as e:
        logger.error(f"Error in comparing answers: {str(e)}")
        raise Exception(f"Failed to compare answers: {str(e)}")
