from pydantic import BaseModel
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    BaseMessage,
    ToolMessage,
)
from langchain_core.tools import tool
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain import hub
from typing import Literal
import os
import logging
import json

from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()
# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


# LLM ì´ˆê¸°í™”
logger.info("Starting interview chain initialization...")
llm = ChatOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"), temperature=0.7, model_name="gpt-4o-mini"
)

@tool
def classify_input(input_data: str) -> str:
    """ì‚¬ìš©ì ì…ë ¥ê³¼, ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì…ë ¥ì´ ì–´ë–¤ í˜•ì‹ì¸ì§€ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    try:
        data = json.loads(input_data)
        request = data.get("request", "")
        chat_history = data.get("chat_history", "")
    except json.JSONDecodeError:
        # JSONì´ ì•„ë‹Œ ê²½ìš° ì§ì ‘ ì²˜ë¦¬
        request = input_data
        chat_history = ""

    classify_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒ ì…ë ¥ì´ ì–´ë–¤ ìœ í˜•ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”: 
        - ìì†Œì„œ ì…ë ¥ (resume)
        - ì§ˆë¬¸ ìš”ì²­ (question)
        - ê¼¬ë¦¬ì§ˆë¬¸ ìš”ì²­ (followup)
        - ëª¨ë²” ë‹µë³€ ìš”ì²­ (modelAnswer)
        - ë‹µë³€ ìš”ì²­ (answer)
        - ê·¸ ì™¸ ì¼ë°˜ í…ìŠ¤íŠ¸ (other)

        ì…ë ¥:
        ì‚¬ìš©ì ì…ë ¥: {request}
        ì´ì „ ëŒ€í™” ë‚´ìš©: {chat_history}

        í˜•ì‹: resume, question, followup, modelAnswer, answer, other ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•˜ì„¸ìš”.
        """
    )
    chain = classify_prompt | llm | StrOutputParser()
    return chain.invoke({"request": request, "chat_history": chat_history})

@tool
def generate_question_reasoning(data: str) -> str:
    """ìì†Œì„œ, JD, íšŒì‚¬ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ì´ìœ (Reasoning)ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤."""
    try:
        parsed_data = json.loads(data)
        resume = parsed_data.get("resume", "")
        jd = parsed_data.get("jd", "")
        company = parsed_data.get("company", "")
        persona = parsed_data.get("persona", "")
    except json.JSONDecodeError:
        return "ì…ë ¥ ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”."

    reasoning_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒì€ ì§€ì›ìì˜ ìì†Œì„œ, JD(ì§ë¬´ê¸°ìˆ ì„œ), íšŒì‚¬ ì •ë³´, ê·¸ë¦¬ê³  ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ì…ë‹ˆë‹¤:

        ìê¸°ì†Œê°œì„œ:
        {resume}

        JD:
        {jd}

        íšŒì‚¬ ì •ë³´:
        {company}

        ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜:
        {persona}

        ë‹¹ì‹ ì€ ìœ„ í˜ë¥´ì†Œë‚˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
        ë‹¤ìŒì„ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ ìƒì„±ì˜ ì´ìœ (Reasoning)ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

        1. íšŒì‚¬ ì¸ì¬ìƒì— ë¶€í•©í•˜ëŠ” ì„±ê²©/ì—­ëŸ‰/í–‰ë™ì„ ìì†Œì„œì—ì„œ ì–¼ë§ˆë‚˜ í™•ì¸í•  ìˆ˜ ìˆëŠ”ê°€?
        2. JDì—ì„œ ìš”êµ¬í•˜ëŠ” ìê²©ìš”ê±´, ê¸°ìˆ , ê²½í—˜ê³¼ ìì†Œì„œê°€ ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ê°€?
        3. ë¶€ì¡±í•˜ê±°ë‚˜ í™•ì¸ì´ í•„ìš”í•œ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€?
        4. ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ì˜ ì‹œê°ê³¼ ë§íˆ¬, ì„±ê²©ì„ ë°˜ì˜í•œ ë¶„ì„

        Reasoningì„ ëª…í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        
        ì¶œë ¥ ì˜ˆì‹œ:
        - (ë…¼ë¦¬ì  ìŠ¤íƒ€ì¼) "JDì—ì„œ ìš”êµ¬í•œ í˜‘ì—… ê²½í—˜ì´ ìì†Œì„œì— êµ¬ì²´ì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ì§€ ì•Šì•„, ì‹¤ì œë¡œ íŒ€ í”„ë¡œì íŠ¸ë¥¼ ì£¼ë„í•œ ê²½í—˜ì´ ìˆì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¶ë‹¤."
        - (í˜¸ê¸°ì‹¬ ë§ì€ ìŠ¤íƒ€ì¼) "ì§€ì›ìëŠ” ë°ì´í„° ë¶„ì„ ê²½í—˜ì´ ìˆë‹¤ê³  í–ˆì§€ë§Œ ì–´ë–¤ íˆ´ì„ ì‚¬ìš©í–ˆëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤. í”„ë¡œì íŠ¸ ë§¥ë½ì„ ë” ë“£ê³  ì‹¶ë‹¤."
        """
    )

    chain = reasoning_prompt | llm | StrOutputParser()
    return chain.invoke({
        "resume": resume, 
        "jd": jd, 
        "company": company, 
        "persona": persona
    })

@tool
def generate_question_acting(reasoning: str) -> str:
    """Reasoningì— ê¸°ë°˜í•˜ì—¬ ì‹¤ì œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    acting_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒì€ ë©´ì ‘ ì§ˆë¬¸ì„ ë§Œë“¤ê¸° ìœ„í•œ Reasoningì…ë‹ˆë‹¤:

        {reasoning}

        ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë©´ì ‘ ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.
        ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ê³  ë‹µë³€ ê°€ëŠ¥í•˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”.
        
        ì§ˆë¬¸:
        """
    )

    chain = acting_prompt | llm | StrOutputParser()
    return chain.invoke({"reasoning": reasoning})

@tool
def generate_followup_reasoning(input_data: str) -> str:
    """ì§€ì›ìì˜ ë‹µë³€ì— ëŒ€í•œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•œ Reasoningì„ ë„ì¶œí•©ë‹ˆë‹¤."""
    try:
        data = json.loads(input_data)
        chat_history = data.get("chat_history", "")
        input_text = data.get("input_text", "")
    except json.JSONDecodeError:
        input_text = input_data
        chat_history = ""

    reasoning_prompt = PromptTemplate.from_template(
        """
        ì•„ë˜ëŠ” AI ë©´ì ‘ ì‹œìŠ¤í…œì—ì„œ ì§€ê¸ˆê¹Œì§€ ì§„í–‰ëœ ëŒ€í™”ì…ë‹ˆë‹¤:

        ëŒ€í™” ì´ë ¥:
        {chat_history}

        í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì›ìì˜ ë‹µë³€:
        "{input_text}"

        ì´ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•˜ê¸° ìœ„í•œ ë©´ì ‘ê´€ì˜ Reasoningì„ ì‘ì„±í•˜ì„¸ìš”.
        ì§€ì›ìì˜ ë‹µë³€ì—ì„œ êµ¬ì²´ì ì´ì§€ ì•Šì€ ë¶€ë¶„, ì¶”ê°€ ì„¤ëª…ì´ í•„ìš”í•œ í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ë‚´ì„¸ìš”.
        """
    )

    chain = reasoning_prompt | llm | StrOutputParser()
    return chain.invoke({
        "chat_history": chat_history,
        "input_text": input_text
    })

@tool
def generate_followup_acting(input_data: str) -> str:
    """Reasoningê³¼ ì§€ì›ì ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        data = json.loads(input_data)
        reasoning = data.get("reasoning", "")
        input_text = data.get("input_text", "")
    except json.JSONDecodeError:
        return "ì…ë ¥ ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."

    prompt = PromptTemplate.from_template(
        """
        [ì§€ì›ì ë‹µë³€]
        {input_text}

        [ë©´ì ‘ê´€ì˜ Reasoning]
        {reasoning}

        ìœ„ Reasoningì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  êµ¬ì²´ì ì¸ ê¼¬ë¦¬ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        """
    )

    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"reasoning": reasoning, "input_text": input_text})


class AssessmentResult(BaseModel):
    logicScore: int
    jobFitScore: int
    coreValueFitScore: int
    communicationScore: int
    averageScore: float


@tool
def evaluate_answer(data):
    """ì§€ì›ì ë‹µë³€ì„ í‰ê°€"""

    parser = JsonOutputParser(pydantic_object=AssessmentResult)
    
    try:
        parsed_data = json.loads(data)
        resume = parsed_data.get("resume", "")
        jd = parsed_data.get("jd", "")
        company = parsed_data.get("company", "")
        question = parsed_data.get("question", "")
        answer = parsed_data.get("answer", "")
        persona = parsed_data.get("persona", "")
    except json.JSONDecodeError:
        return {"error": "ì…ë ¥ ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."}

    assessment_prompt = PromptTemplate(
        input_variables=["resume", "jd", "company", "question", "answer", "persona"],
        template="""
        ì—­í• : ë©´ì ‘ê´€ìœ¼ë¡œì„œ ì§€ì›ìì˜ ë‹µë³€ì„ í‰ê°€í•©ë‹ˆë‹¤.

        ì§ë¬´ ì„¤ëª…:
        {jd}

        ì´ë ¥ì„œ:
        {resume}

        íšŒì‚¬ ì •ë³´:
        {company}

        ì§ˆë¬¸: 
        {question}
        
        ì§€ì›ìì˜ ë‹µë³€:
        {answer}

        ë©´ì ‘ê´€ ì •ë³´:
        {persona}
        
        ë‹¤ìŒ 4ê°œ í•­ëª©ì„ 0-10ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
        1. ë…¼ë¦¬ì„± (logicScore): ë‹µë³€ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„±ê³¼ êµ¬ì¡°
        2. ì§ë¬´ì í•©ì„± (jobFitScore): JD ìš”êµ¬ì‚¬í•­ê³¼ì˜ ë¶€í•©ë„
        3. í•µì‹¬ê°€ì¹˜ ë¶€í•©ì„± (coreValueFitScore): íšŒì‚¬ ê°€ì¹˜ì™€ì˜ ì¼ì¹˜ë„
        4. ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëŠ¥ë ¥ (communicationScore): ì˜ì‚¬ì†Œí†µ ëª…í™•ì„±

        {format_instructions}
        """,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )

    chain = assessment_prompt | llm | parser
    
    try:
        result = chain.invoke({
            "jd": jd,
            "resume": resume,
            "company": company,
            "question": question,
            "answer": answer,
            "persona": persona,
        })
        return result
    except Exception as e:
        logger.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": "í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

tools = [
    classify_input,
    generate_question_reasoning,
    generate_question_acting,
]


def parse_role_from_message(message: BaseMessage) -> Literal["assistant", "human", "system", "tool", "unknown"]:
    """ë©”ì‹œì§€ì—ì„œ ì—­í• ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if isinstance(message, AIMessage):
        return "assistant"
    elif isinstance(message, HumanMessage):
        return "human"
    elif isinstance(message, SystemMessage):
        return "system"
    elif isinstance(message, ToolMessage):
        return "tool"
    else:
        return "unknown"


# ReAct í”„ë¡¬í”„íŠ¸ (langchain hubì—ì„œ ê°€ì ¸ì˜¤ê¸°)
try:
    # LangChain Hubì—ì„œ í‘œì¤€ ReAct í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
    prompt = hub.pull("hwchase17/react")
except:
    # Hubë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    prompt = PromptTemplate.from_template(
        """
        ë‹¹ì‹ ì€ ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ íš¨ê³¼ì ì¸ ë©´ì ‘ì„ ì§„í–‰í•˜ì„¸ìš”:

        {tools}

        ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:

        Question: ë‹µë³€í•´ì•¼ í•  ì…ë ¥ ì§ˆë¬¸
        Thought: ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ìƒê°í•˜ì„¸ìš”
        Action: ìˆ˜í–‰í•  ì‘ì—…, ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤: [{tool_names}]
        Action Input: ì‘ì—…ì— ëŒ€í•œ ì…ë ¥
        Observation: ì‘ì—…ì˜ ê²°ê³¼
        ... (ì´ Thought/Action/Action Input/Observationì€ Në²ˆ ë°˜ë³µë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)
        Thought: ì´ì œ ìµœì¢… ë‹µë³€ì„ ì•Œì•˜ìŠµë‹ˆë‹¤
        Final Answer: ì›ë˜ ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€

        ì‹œì‘!

Question: {input}
Thought: {agent_scratchpad}
"""
)

# LLM ì´ˆê¸°í™”
logger.info("Starting interview chain initialization...")
llm = ChatOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    temperature=0.7,
    model_name="gpt-4o",
    verbose=True,
)

# memory = MemorySaver()
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent, 
    tools=tools, 
    verbose=True,
    max_iterations=3,  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì œí•œ
    handle_parsing_errors=True  # íŒŒì‹± ì˜¤ë¥˜ ì²˜ë¦¬
)

class InterviewAgent:
    """ë©´ì ‘ê´€ Agent í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.agent_executor = agent_executor
        self.logger = logger
    
    def run_interview(self, input_data: dict) -> str:
        """ë©´ì ‘ ì§„í–‰"""
        try:
            # ì…ë ¥ ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            input_json = json.dumps(input_data, ensure_ascii=False)
            
            # Agent ì‹¤í–‰
            result = self.agent_executor.invoke({
                "input": input_json
            })
            
            return result.get("output", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"ë©´ì ‘ ì§„í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def generate_question(self, resume: str, jd: str, company: str, persona: str = "") -> str:
        """ì§ˆë¬¸ ìƒì„±"""
        input_data = {
            "action": "generate_question",
            "resume": resume,
            "jd": jd,
            "company": company,
            "persona": persona
        }
        return self.run_interview(input_data)
    
    def generate_followup(self, chat_history: str, current_answer: str) -> str:
        """ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±"""
        input_data = {
            "action": "generate_followup",
            "chat_history": chat_history,
            "current_answer": current_answer
        }
        return self.run_interview(input_data)
    
    def evaluate_response(self, resume: str, jd: str, company: str, 
                         question: str, answer: str, persona: str = "") -> dict:
        """ë‹µë³€ í‰ê°€"""
        input_data = {
            "action": "evaluate",
            "resume": resume,
            "jd": jd,
            "company": company,
            "question": question,
            "answer": answer,
            "persona": persona
        }
        result = self.run_interview(input_data)
        try:
            return json.loads(result)
        except:
            return {"error": "í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}


def run_interview_question_pipeline(resume: str, jd: str, company: str) -> str:
    return agent_executor.invoke(
        f"generate_interview_question(resume='{resume}', jd='{jd}', company='{company}')"
    )


def get_initial_message_chain():
    initial_prompt = PromptTemplate(
        template="""
        ë‹¹ì‹ ì€ AI ë©´ì ‘ ì‹œë®¬ë ˆì´í„°ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë©´ì ‘ì„ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì•ˆë‚´í•˜ëŠ” ì²« ë©”ì‹œì§€ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        ì¡°ê±´:
        - ì‚¬ìš©ìê°€ ë‹¹ì‹ ì´ AI ë©´ì ‘ê´€ì„ì„ ì´í•´í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        - ì‚¬ìš©ìê°€ ìì†Œì„œ(ì´ë ¥ì„œ)ë¥¼ ë¨¼ì € ì…ë ¥í•´ì•¼ í•œë‹¤ëŠ” ì ì„ ê¼­ ì„¤ëª…í•˜ì„¸ìš”.
        - ë„ˆë¬´ ë”±ë”±í•˜ê±°ë‚˜ ê¸°ê³„ì ì´ì§€ ì•Šê³ , ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ëŠ” ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        - í•œ ë¬¸ë‹¨(2~4ì¤„) ë¶„ëŸ‰ì˜ ì§§ì€ ì¸ì‚¿ë§ë¡œ ì‘ì„±í•˜ê³ , ì§§ì€ í˜¸í¡ìœ¼ë¡œ ì¤„ë°”ê¿ˆ(\n)í•˜ì„¸ìš”.

        ì˜ˆì‹œ1:
        ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì—¬ëŸ¬ë¶„ì˜ ë©´ì ‘ì„ ë„ì™€ë“œë¦´ AI ì‹œë®¬ë ˆì´í„°ì…ë‹ˆë‹¤. ğŸ‘‹
        ì €ëŠ” ìì†Œì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆìƒ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³ , 
        ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        ë©´ì ‘ì„ ì‹œì‘í•˜ë ¤ë©´ ë¨¼ì € ë³¸ì¸ì˜ ì´ë ¥ì„œ(ìê¸°ì†Œê°œì„œ)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.
        ì…ë ¥í•˜ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©´ì ‘ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤.

        ì˜ˆì‹œ2:
        AI ë©´ì ‘ ì‹œë®¬ë ˆì´í„°ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤ :) 
        ë¨¼ì € ìì†Œì„œ(ì´ë ¥ì„œ)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.
        í•´ë‹¹ ë‚´ìš©ì„ ë¶„ì„í•´ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©´ì ‘ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤.

        ì˜ˆì‹œ3:
        ë°˜ê°‘ìŠµë‹ˆë‹¤! ì§€ê¸ˆë¶€í„° AIê°€ ë©´ì ‘ê´€ ì—­í• ì„ ëŒ€ì‹ í•´ë“œë¦´ê²Œìš”. 
        ì‹œì‘í•˜ë ¤ë©´ ë³¸ì¸ì˜ ì´ë ¥ì„œ(ìê¸°ì†Œê°œì„œ)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.
        ì…ë ¥í•˜ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë©´ì ‘ì„ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤. ğŸ”¥
        """
    )
    initial_chain = LLMChain(llm=llm, prompt=initial_prompt, output_key="result")
    return initial_chain


def get_reranking_model_answer_chain():
    reranking_prompt = PromptTemplate(
        input_variables=[
            "resume",
            "jd",
            "company_infos",
            "question",
            "prev_question_answer_pairs",
        ],
        template="""
            ì—­í• :
            ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            ì´ ë‹µë³€ì€ íšŒì‚¬ì˜ ê°€ì¹˜ê´€, ì§ë¬´ ìš”êµ¬ì‚¬í•­, ê·¸ë¦¬ê³  ì´ë ¥ì„œì˜ ë‚´ìš©ì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

            ìƒí™©:
            {company_infos}

            ì´ë ¥ì„œ:
            {resume}

            ì§ë¬´ ì„¤ëª…:
            {jd}

            ì´ì „ ì§ˆë¬¸/ë‹µë³€ ìŒë“¤:
            {prev_question_answer_pairs}

            í˜„ì¬ ì§ˆë¬¸:
            {question}

            ë‹¤ìŒ ë‹¨ê³„ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:
            1. íšŒì‚¬ì˜ ê°€ì¹˜ê´€ê³¼ ì§ë¬´ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„
            2. ì´ë ¥ì„œì—ì„œ ê´€ë ¨ëœ ê²½í—˜ê³¼ ì—­ëŸ‰ì„ ì°¾ì•„ ì—°ê²°
            3. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ êµ¬ì„±
            4. êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì˜ˆì‹œë¥¼ í¬í•¨
            5. íšŒì‚¬ì˜ ê°€ì¹˜ê´€ê³¼ ë¶€í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ë§ˆë¬´ë¦¬

            ë‹µë³€ì€ í•œê¸€ë¡œ ìƒì„±í•´ì•¼ í•œë‹¤.
            """,
    )

    reranking_chain = LLMChain(llm=llm, prompt=reranking_prompt, output_key="result")
    return reranking_chain


def compare_model_answers(original_answer: str, reranked_answer: str) -> dict:
    """ë‘ ëª¨ë¸ ë‹µë³€ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜"""
    comparison_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
ë‹¹ì‹ ì€ ë‘ ê°œì˜ ë©´ì ‘ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ì¸ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ë‘ ë‹µë³€ì„ ë¹„êµí•˜ì„¸ìš”:
1. êµ¬ì²´ì„±: ì˜ˆì‹œì™€ ê²½í—˜ì´ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œë˜ì—ˆëŠ”ê°€?
2. ê´€ë ¨ì„±: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ë¬´ ìš”êµ¬ì‚¬í•­ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ê°€?
3. êµ¬ì¡°í™”: ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ë…¼ë¦¬ì ì´ê³  ëª…í™•í•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ê°€?
4. íšŒì‚¬ ì í•©ì„±: ë‹µë³€ì´ íšŒì‚¬ì˜ ê°€ì¹˜ê´€ê³¼ ë¬¸í™”ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ê°€?
5. ì „ë¬¸ì„±: ë‹µë³€ì´ ì§ë¬´ ê´€ë ¨ ì§€ì‹ê³¼ ì—­ëŸ‰ì„ ì–¼ë§ˆë‚˜ ì˜ ë³´ì—¬ì£¼ëŠ”ê°€?

ê° ê¸°ì¤€ë³„ë¡œ
- ì›ë³¸ ë‹µë³€ê³¼ reranking ë‹µë³€ ê°ê° 1~10ì ìœ¼ë¡œ í‰ê°€
- ê°„ë‹¨í•œ ì„¤ëª…
- ì–´ë–¤ ë‹µë³€ì´ ë” ë‚˜ì€ì§€

ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì„¸ìš”(ì¶”ê°€ ì„¤ëª…, ì½”ë“œë¸”ë¡ ë“± ê¸ˆì§€):
{{
    "specificity": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "relevance": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "structure": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "company_fit": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "expertise": {{
        "original_score": number,
        "reranked_score": number,
        "explanation": string,
        "better_answer": "original" ë˜ëŠ” "reranked"
    }},
    "overall": {{
        "original_total": number,
        "reranked_total": number,
        "better_answer": "original" ë˜ëŠ” "reranked",
        "summary": string
    }}
}}
ëª¨ë“  ì„¤ëª…ê³¼ ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
            ),
            (
                "user",
                """ë‹¤ìŒ ë‘ ë‹µë³€ì„ ë¹„êµí•˜ì„¸ìš”:

ì›ë³¸ ë‹µë³€:
{original_answer}

Reranking ë‹µë³€:
{reranked_answer}""",
            ),
        ]
    )

    chain = comparison_prompt | llm | StrOutputParser()

    try:
        result = chain.invoke(
            {"original_answer": original_answer, "reranked_answer": reranked_answer}
        )
        # ì½”ë“œë¸”ë¡ ë“± ì œê±°
        import re

        result_str = result.strip()
        # ```json ... ``` ë˜ëŠ” ``` ... ``` ì œê±°
        result_str = re.sub(
            r"^```(?:json)?|```$", "", result_str, flags=re.MULTILINE
        ).strip()
        comparison_result = json.loads(result_str)
        return comparison_result
    except Exception as e:
        logger.error(f"Error in comparing answers: {str(e)}")
        raise Exception(f"Failed to compare answers: {str(e)}")
